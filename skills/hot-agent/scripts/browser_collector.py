#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµè§ˆå™¨æ–°é—»é‡‡é›†å™¨ - Browser-based News Collector

ä½¿ç”¨ OpenClaw æµè§ˆå™¨å·¥å…·ä»æ–°é—»ç½‘ç«™ç›´æ¥é‡‡é›†çƒ­ç‚¹ä¿¡æ¯
æ”¯æŒå¤šç§çƒ­ç‚¹æ¦œå•ï¼šå¾®åšçƒ­æœã€çŸ¥ä¹çƒ­æ¦œã€å“”å“©å“”å“©çƒ­é—¨ç­‰

ä½œè€…: OpenClaw Agent
åˆ›å»ºæ—¶é—´: 2026-02-10
"""

import os
import sys
import json
import re
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from abc import ABC, abstractmethod

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class BrowserNews:
    """æµè§ˆå™¨é‡‡é›†çš„æ–°é—»"""
    title: str
    url: str
    source: str
    hot_level: Optional[str] = None  # çƒ­åº¦ç­‰çº§
    rank: Optional[int] = None  # æ’å
    publish_time: Optional[str] = None
    description: Optional[str] = None
    category: Optional[str] = None
    raw_data: Dict = field(default_factory=dict)


class BaseNewsExtractor(ABC):
    """æ–°é—»æå–å™¨åŸºç±»"""

    def __init__(self, source_name: str):
        self.source_name = source_name
        self.driver = None

    @abstractmethod
    def get_source_url(self) -> str:
        """è·å–æºURL"""
        pass

    @abstractmethod
    def extract_news(self, page_content: Dict) -> List[BrowserNews]:
        """ä»é¡µé¢å†…å®¹æå–æ–°é—»"""
        pass

    def get_hot_level(self, rank: int) -> str:
        """æ ¹æ®æ’åè¿”å›çƒ­åº¦ç­‰çº§"""
        if rank <= 3:
            return "ğŸ”¥ğŸ”¥ğŸ”¥"
        elif rank <= 10:
            return "ğŸ”¥ğŸ”¥"
        elif rank <= 20:
            return "ğŸ”¥"
        else:
            return "ğŸ“ˆ"


class WeiboExtractor(BaseNewsExtractor):
    """å¾®åšçƒ­æœæå–å™¨"""

    def __init__(self):
        super().__init__("å¾®åš")

    def get_source_url(self) -> str:
        return "https://weibo.com/çƒ­æœ"

    def extract_news(self, page_content: Dict) -> List[BrowserNews]:
        news_list = []

        # ä»é¡µé¢å†…å®¹ä¸­æå–
        if 'links' in page_content:
            for i, link in enumerate(page_content['links'][:20]):
                title = link.get('text', '')
                url = link.get('href', '')

                if title and url and 'å¾®åš' not in title:
                    news = BrowserNews(
                        title=title[:50],
                        url=url,
                        source="å¾®åšçƒ­æœ",
                        rank=i + 1,
                        hot_level=self.get_hot_level(i + 1),
                        category="ç»¼åˆ"
                    )
                    news_list.append(news)

        return news_list


class ZhihuExtractor(BaseNewsExtractor):
    """çŸ¥ä¹çƒ­æ¦œæå–å™¨"""

    def __init__(self):
        super().__init__("çŸ¥ä¹")

    def get_source_url(self) -> str:
        return "https://www.zhihu.com/hot"

    def extract_news(self, page_content: Dict) -> List[BrowserNews]:
        news_list = []

        # ä»é¡µé¢å…ƒç´ ä¸­æå–
        if 'headings' in page_content:
            for i, heading in enumerate(page_content['headings'][:20]):
                title = heading.get('text', '')
                url = heading.get('href', '')

                if title and url:
                    news = BrowserNews(
                        title=title[:80],
                        url=url,
                        source="çŸ¥ä¹çƒ­æ¦œ",
                        rank=i + 1,
                        hot_level=self.get_hot_level(i + 1),
                        category="ç»¼åˆ"
                    )
                    news_list.append(news)

        return news_list


class BilibiliExtractor(BaseNewsExtractor):
    """å“”å“©å“”å“©çƒ­é—¨æå–å™¨"""

    def __init__(self):
        super().__init__("å“”å“©å“”å“©")

    def get_source_url(self) -> str:
        return "https://www.bilibili.com/v/rank/all"

    def extract_news(self, page_content: Dict) -> List[BrowserNews]:
        news_list = []

        # ä»è§†é¢‘åˆ—è¡¨æå–
        if 'items' in page_content:
            for i, item in enumerate(page_content['items'][:20]):
                title = item.get('title', '')
                url = item.get('link', '')

                if title and url:
                    views = item.get('views', '')
                    news = BrowserNews(
                        title=f"[Bç«™] {title[:50]}",
                        url=url,
                        source="å“”å“©å“”å“©",
                        rank=i + 1,
                        hot_level=f"ğŸ‘€ {views}" if views else "ğŸ“º",
                        category="è§†é¢‘"
                    )
                    news_list.append(news)

        return news_list


class DoubanExtractor(BaseNewsExtractor):
    """è±†ç“£çƒ­é—¨æå–å™¨"""

    def __init__(self):
        super().__init__("è±†ç“£")

    def get_source_url(self) -> str:
        return "https://movie.douban.com/"

    def extract_news(self, page_content: Dict) -> List[BrowserNews]:
        news_list = []

        if 'movies' in page_content:
            for i, movie in enumerate(page_content['movies'][:10]):
                title = movie.get('title', '')
                url = movie.get('url', '')
                rating = movie.get('rating', '')

                if title and url:
                    news = BrowserNews(
                        title=f"[ç”µå½±] {title}",
                        url=url,
                        source="è±†ç“£",
                        rank=i + 1,
                        hot_level=f"â­ {rating}" if rating else "ğŸ¬",
                        category="å½±è§†"
                    )
                    news_list.append(news)

        return news_list


class BrowserNewsCollector:
    """æµè§ˆå™¨æ–°é—»é‡‡é›†å™¨"""

    def __init__(self):
        self.extractors = {
            'weibo': WeiboExtractor(),
            'zhihu': ZhihuExtractor(),
            'bilibili': BilibiliExtractor(),
            'douban': DoubanExtractor(),
        }
        self.logger = logging.getLogger(__name__)

    def collect_from_source(self, source: str, browser_output: Dict) -> List[BrowserNews]:
        """ä»ç‰¹å®šæ¥æºé‡‡é›†æ–°é—»"""
        if source not in self.extractors:
            self.logger.warning(f"æœªçŸ¥çš„æ–°é—»æº: {source}")
            return []

        extractor = self.extractors[source]
        try:
            news_list = extractor.extract_news(browser_output)
            self.logger.info(f"ä» {source} é‡‡é›†åˆ° {len(news_list)} æ¡æ–°é—»")
            return news_list
        except Exception as e:
            self.logger.error(f"ä» {source} é‡‡é›†å¤±è´¥: {e}")
            return []

    def collect_all_sources(self, browser_outputs: Dict[str, Dict]) -> List[BrowserNews]:
        """ä»å¤šä¸ªæ¥æºé‡‡é›†æ–°é—»"""
        all_news = []

        for source, output in browser_outputs.items():
            if output:
                news = self.collect_from_source(source, output)
                all_news.extend(news)

        self.logger.info(f"å…±é‡‡é›†åˆ° {len(all_news)} æ¡æ–°é—»")
        return all_news

    def merge_with_rss(self, browser_news: List[BrowserNews], rss_news: List) -> List[Dict]:
        """åˆå¹¶æµè§ˆå™¨é‡‡é›†å’ŒRSSé‡‡é›†çš„æ–°é—»"""
        merged = []

        # æ·»åŠ æµè§ˆå™¨æ–°é—»
        for news in browser_news:
            merged.append({
                'title': news.title,
                'url': news.url,
                'source': news.source,
                'hot_level': news.hot_level,
                'rank': news.rank,
                'category': news.category,
                'collection_method': 'browser'
            })

        # æ·»åŠ RSSæ–°é—»
        for news in rss_news:
            merged.append({
                'title': getattr(news, 'title', str(news)),
                'url': getattr(news, 'url', ''),
                'source': getattr(news, 'source', 'RSS'),
                'hot_level': getattr(news, 'heat_score', 0),
                'rank': None,
                'category': getattr(news, 'category', 'ç»¼åˆ'),
                'collection_method': 'rss'
            })

        return merged


def demo():
    """æ¼”ç¤º"""
    print("=" * 70)
    print("æµè§ˆå™¨æ–°é—»é‡‡é›†å™¨æ¼”ç¤º")
    print("=" * 70)

    collector = BrowserNewsCollector()

    print("\næ”¯æŒçš„æ–°é—»æº:")
    for name in collector.extractors:
        print(f"  - {name}")

    print("\næ³¨æ„: å®é™…ä½¿ç”¨æ—¶éœ€è¦å…ˆé€šè¿‡ browser å·¥å…·è®¿é—®ç½‘é¡µ")
    print("ç„¶åä½¿ç”¨ snapshot è·å–é¡µé¢ç»“æ„ï¼Œå†æå–æ–°é—»")

    print("\nä½¿ç”¨æµç¨‹:")
    print("  1. browser action=start profile=openclaw")
    print("  2. browser action=open targetUrl=<æ–°é—»æºURL>")
    print("  3. browser action=snapshot")
    print("  4. ä½¿ç”¨ BrowserNewsCollector è§£æç»“æœ")

    print("\n" + "=" * 70)


if __name__ == "__main__":
    demo()
