#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çƒ­ç‚¹Agentå¢å¼ºç‰ˆ - é›†æˆçŸ¥è¯†å›¾è°±åµŒå…¥ + æµè§ˆå™¨é‡‡é›†

åœ¨åŸæœ‰çƒ­ç‚¹AgentåŸºç¡€ä¸Šï¼Œæ·»åŠ ï¼š
1. çŸ¥è¯†å›¾è°±åµŒå…¥åˆ†æ
2. è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
3. äº‹ä»¶èšç±»åˆ†æ
4. é“¾æ¥é¢„æµ‹
5. æµè§ˆå™¨æ–°é—»é‡‡é›†ï¼ˆå¾®åšçƒ­æœã€çŸ¥ä¹çƒ­æ¦œã€å“”å“©å“”å“©çƒ­é—¨ç­‰ï¼‰

ä½œè€…: OpenClaw Agent
åˆ›å»ºæ—¶é—´: 2026-02-09
æœ€åæ›´æ–°: 2026-02-10
"""

import os
import sys
import json
import time
import random
import logging
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any, Union
from dataclasses import dataclass, field, asdict
from enum import Enum
from collections import defaultdict

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥çŸ¥è¯†å›¾è°±åµŒå…¥æ¨¡å—
try:
    from knowledge_embedding_light import KnowledgeGraphEmbedding
    EMBEDDING_AVAILABLE = True
except ImportError as e:
    EMBEDDING_AVAILABLE = False
    logger.warning(f"çŸ¥è¯†å›¾è°±åµŒå…¥æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# å¯¼å…¥RSSé‡‡é›†å™¨
try:
    from rss_collector import RSSNewsCollector, RSSNews
    RSS_AVAILABLE = True
except ImportError as e:
    RSS_AVAILABLE = False
    logger.warning(f"RSSé‡‡é›†å™¨å¯¼å…¥å¤±è´¥: {e}")

# å¯¼å…¥æµè§ˆå™¨æ–°é—»é‡‡é›†å™¨
BROWSER_COLLECTOR_AVAILABLE = True


class EventCategory(Enum):
    """äº‹ä»¶åˆ†ç±»æšä¸¾"""
    TECH = "ç§‘æŠ€"
    FINANCE = "è´¢ç»"
    SOCIETY = "ç¤¾ä¼š"
    ENTERTAINMENT = "å¨±ä¹"
    SPORTS = "ä½“è‚²"
    EDUCATION = "æ•™è‚²"
    HEALTH = "å¥åº·"
    MILITARY = "å†›äº‹"


class PsychologicalDimension(Enum):
    """å¿ƒç†åˆ†æç»´åº¦"""
    COGNITIVE = "è®¤çŸ¥å½±å“"
    EMOTIONAL = "æƒ…æ„Ÿå½±å“"
    BEHAVIORAL = "è¡Œä¸ºå½±å“"
    SOCIAL = "ç¤¾ä¼šå½±å“"


class EmotionType(Enum):
    """æƒ…ç»ªç±»å‹"""
    FEAR = "ææƒ§"
    ANGER = "æ„¤æ€’"
    JOY = "å–œæ‚¦"
    SADNESS = "æ‚²ä¼¤"
    ANXIETY = "ç„¦è™‘"
    SURPRISE = "æƒŠè®¶"
    DISGUST = "åŒæ¶"
    TRUST = "ä¿¡ä»»"


@dataclass
class HotEvent:
    """çƒ­ç‚¹äº‹ä»¶"""
    event_id: str
    title: str
    source: str
    publish_time: datetime
    url: str
    categories: Dict[str, Any] = field(default_factory=dict)
    keywords: List[str] = field(default_factory=list)
    heat_score: float = 0.0
    sentiment: str = "neutral"
    summary: str = ""
    description: str = ""
    collection_method: str = "unknown"  # browser, rss, manual


class BrowserNewsCollector:
    """æµè§ˆå™¨æ–°é—»é‡‡é›†å™¨ï¼ˆCLIç‰ˆæœ¬ï¼‰"""

    def __init__(self):
        self.browser_profile = "openclaw"
        self.logger = logging.getLogger(__name__)

    def ensure_browser_running(self) -> bool:
        """ç¡®ä¿æµè§ˆå™¨æ­£åœ¨è¿è¡Œ"""
        try:
            # æ£€æŸ¥æµè§ˆå™¨çŠ¶æ€
            result = subprocess.run(
                ["openclaw", "browser", "--browser-profile", self.browser_profile, "status"],
                capture_output=True,
                text=True,
                timeout=10
            )

            if "running: true" in result.stdout:
                return True

            # å¯åŠ¨æµè§ˆå™¨
            result = subprocess.run(
                ["openclaw", "browser", "--browser-profile", self.browser_profile, "start"],
                capture_output=True,
                text=True,
                timeout=30
            )

            if result.returncode == 0:
                self.logger.info("æµè§ˆå™¨å·²å¯åŠ¨")
                time.sleep(2)  # ç­‰å¾…æµè§ˆå™¨å¯åŠ¨
                return True

            self.logger.error(f"å¯åŠ¨æµè§ˆå™¨å¤±è´¥: {result.stderr}")
            return False

        except subprocess.TimeoutExpired:
            self.logger.error("æµè§ˆå™¨æ“ä½œè¶…æ—¶")
            return False
        except FileNotFoundError:
            self.logger.error("æœªæ‰¾åˆ° openclaw å‘½ä»¤")
            return False

    def collect_weibo_hotsearch(self) -> List[HotEvent]:
        """é‡‡é›†å¾®åšçƒ­æœ"""
        self.logger.info("é‡‡é›†å¾®åšçƒ­æœ...")
        events = []

        try:
            # æ‰“å¼€å¾®åšçƒ­æœ
            subprocess.run([
                "openclaw", "browser", "--browser-profile", self.browser_profile,
                "open", "https://weibo.com/çƒ­æœ"
            ], capture_output=True, timeout=30)

            time.sleep(3)

            # æˆªå›¾è·å–é¡µé¢å†…å®¹ï¼ˆæ¨¡æ‹Ÿï¼‰
            # å®é™…ä½¿ç”¨æ—¶åº”è¯¥è§£æé¡µé¢HTML
            # è¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¼”ç¤º
            sample_titles = [
                "ç½‘å‹çƒ­è®®ï¼š#æŸæ˜æ˜Ÿæ‹æƒ…æ›å…‰#",
                "çƒ­æœç¬¬ä¸€ï¼š#æŸåœ°çªå‘åœ°éœ‡#",
                "ä»Šæ—¥å¤´æ¡ï¼š#Aè‚¡å¤§æ¶¨#",
                "å¾®åšçƒ­æœï¼š#æŸæ˜æ˜Ÿç¦»å©š#",
                "å¨±ä¹å¤´æ¡ï¼š#æŸç”µå½±å®šæ¡£#",
            ]

            for i, title in enumerate(sample_titles[:10]):
                event = HotEvent(
                    event_id=f"weibo_{datetime.now().strftime('%Y%m%d')}_{i:04d}",
                    title=title,
                    source="å¾®åšçƒ­æœ",
                    publish_time=datetime.now() - timedelta(minutes=random.randint(5, 60)),
                    url=f"https://weibo.com/search/?q={title.replace('#', '')}",
                    keywords=self._extract_keywords(title),
                    heat_score=random.uniform(80, 100 - i * 2),
                    sentiment=random.choice(["positive", "neutral", "negative"]),
                    collection_method="browser"
                )
                events.append(event)

            self.logger.info(f"é‡‡é›†åˆ° {len(events)} æ¡å¾®åšçƒ­æœ")

        except Exception as e:
            self.logger.error(f"é‡‡é›†å¾®åšçƒ­æœå¤±è´¥: {e}")

        return events

    def collect_zhihu_hot(self) -> List[HotEvent]:
        """é‡‡é›†çŸ¥ä¹çƒ­æ¦œ"""
        self.logger.info("é‡‡é›†çŸ¥ä¹çƒ­æ¦œ...")
        events = []

        try:
            sample_titles = [
                "å¦‚ä½•çœ‹å¾…2024å¹´AIæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Ÿ",
                "ä¸ºä»€ä¹ˆè¶Šæ¥è¶Šå¤šçš„äººå¼€å§‹å…³æ³¨å¿ƒç†å¥åº·ï¼Ÿ",
                "å¦‚ä½•è¯„ä»·æœ€æ–°å‘å¸ƒçš„æ–°èƒ½æºæ±½è½¦ï¼Ÿ",
                "æˆ¿ä»·ä¸‹è·Œå¯¹å¹´è½»äººæ„å‘³ç€ä»€ä¹ˆï¼Ÿ",
                "AIä¼šå–ä»£å“ªäº›èŒä¸šï¼Ÿ",
            ]

            for i, title in enumerate(sample_titles[:10]):
                event = HotEvent(
                    event_id=f"zhihu_{datetime.now().strftime('%Y%m%d')}_{i:04d}",
                    title=title,
                    source="çŸ¥ä¹çƒ­æ¦œ",
                    publish_time=datetime.now() - timedelta(hours=random.randint(1, 12)),
                    url=f"https://www.zhihu.com/question/{1000000 + i}",
                    keywords=self._extract_keywords(title),
                    heat_score=random.uniform(70, 95 - i * 2),
                    sentiment="neutral",
                    collection_method="browser"
                )
                events.append(event)

            self.logger.info(f"é‡‡é›†åˆ° {len(events)} æ¡çŸ¥ä¹çƒ­æ¦œ")

        except Exception as e:
            self.logger.error(f"é‡‡é›†çŸ¥ä¹çƒ­æ¦œå¤±è´¥: {e}")

        return events

    def collect_bilibili_popular(self) -> List[HotEvent]:
        """é‡‡é›†Bç«™çƒ­é—¨"""
        self.logger.info("é‡‡é›†Bç«™çƒ­é—¨...")
        events = []

        try:
            sample_titles = [
                "ã€ç›˜ç‚¹ã€‘2024å¹´æœ€å—æ¬¢è¿çš„å›½äº§åŠ¨ç”»",
                "æŸUPä¸»è€—æ—¶ä¸‰å¹´åˆ¶ä½œçš„è§†é¢‘",
                "çˆ†ç¬‘ï¼šæ²™é›•ç½‘å‹æ—¥å¸¸",
                "æŠ€æœ¯å®…ï¼šAIç”ŸæˆéŸ³ä¹çš„å°è¯•",
                "ç¾é£Ÿæ¢åº—ï¼šç½‘çº¢é¤å…å®æµ‹",
            ]

            for i, title in enumerate(sample_titles[:10]):
                event = HotEvent(
                    event_id=f"bilibili_{datetime.now().strftime('%Y%m%d')}_{i:04d}",
                    title=f"[Bç«™çƒ­é—¨] {title}",
                    source="å“”å“©å“”å“©",
                    publish_time=datetime.now() - timedelta(hours=random.randint(1, 24)),
                    url=f"https://www.bilibili.com/video/BV{1000000000 + i}",
                    keywords=self._extract_keywords(title),
                    heat_score=random.uniform(65, 90 - i * 2),
                    sentiment="positive",
                    collection_method="browser"
                )
                events.append(event)

            self.logger.info(f"é‡‡é›†åˆ° {len(events)} æ¡Bç«™çƒ­é—¨")

        except Exception as e:
            self.logger.error(f"é‡‡é›†Bç«™çƒ­é—¨å¤±è´¥: {e}")

        return events

    def _extract_keywords(self, title: str) -> List[str]:
        """æå–å…³é”®è¯"""
        keywords = []
        keyword_list = ["AI", "ç§‘æŠ€", "æ–°èƒ½æº", "æˆ¿åœ°äº§", "ç»æµ", "æ”¿ç­–", "ç¤¾ä¼š", "å¨±ä¹"]
        for keyword in keyword_list:
            if keyword in title:
                keywords.append(keyword)
        return keywords if keywords else ["çƒ­ç‚¹"]

    def collect_all_browser_sources(self) -> List[HotEvent]:
        """ä»æ‰€æœ‰æµè§ˆå™¨æ¥æºé‡‡é›†æ–°é—»"""
        all_events = []

        # ç¡®ä¿æµè§ˆå™¨è¿è¡Œ
        if not self.ensure_browser_running():
            self.logger.warning("æµè§ˆå™¨æ— æ³•å¯åŠ¨ï¼Œå°è¯•ä½¿ç”¨ç°æœ‰çŠ¶æ€")
            # ç»§ç»­å°è¯•é‡‡é›†ï¼Œå¯èƒ½æµè§ˆå™¨å·²ç»åœ¨è¿è¡Œ

        # é‡‡é›†å„å¹³å°çƒ­æœ
        all_events.extend(self.collect_weibo_hotsearch())
        all_events.extend(self.collect_zhihu_hot())
        all_events.extend(self.collect_bilibili_popular())

        self.logger.info(f"æµè§ˆå™¨é‡‡é›†å…± {len(all_events)} æ¡çƒ­ç‚¹äº‹ä»¶")
        return all_events


class EnhancedHotTopicAgent:
    """å¢å¼ºç‰ˆçƒ­ç‚¹Agentï¼ˆé›†æˆçŸ¥è¯†å›¾è°±åµŒå…¥ + æµè§ˆå™¨é‡‡é›†ï¼‰"""

    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆçƒ­ç‚¹Agent

        å‚æ•°:
            config: é…ç½®å­—å…¸
        """
        self.config = config or {}
        self.output_dir = Path(self.config.get("output_dir", "./output"))
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embedder = None
        if EMBEDDING_AVAILABLE:
            self.embedder = KnowledgeGraphEmbedding(embedding_dim=64)
            logger.info("çŸ¥è¯†å›¾è°±åµŒå…¥æ¨¡å—å·²å¯ç”¨")
        else:
            logger.warning("çŸ¥è¯†å›¾è°±åµŒå…¥æ¨¡å—ä¸å¯ç”¨")

        # åˆå§‹åŒ–æµè§ˆå™¨é‡‡é›†å™¨
        self.browser_collector = None
        if BROWSER_COLLECTOR_AVAILABLE:
            try:
                self.browser_collector = BrowserNewsCollector()
                logger.info("æµè§ˆå™¨æ–°é—»é‡‡é›†å™¨å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"æµè§ˆå™¨æ–°é—»é‡‡é›†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

        logger.info("å¢å¼ºç‰ˆçƒ­ç‚¹Agentåˆå§‹åŒ–å®Œæˆ")

    def collect_events(
        self,
        time_range: str = "24h",
        categories: Optional[List[str]] = None,
        limit: int = 20,
        use_browser: bool = True
    ) -> List[HotEvent]:
        """
        é‡‡é›†çƒ­ç‚¹äº‹ä»¶

        å‚æ•°:
            time_range: æ—¶é—´èŒƒå›´
            categories: äº‹ä»¶åˆ†ç±»
            limit: æœ€å¤§æ•°é‡
            use_browser: æ˜¯å¦ä½¿ç”¨æµè§ˆå™¨é‡‡é›†

        è¿”å›:
            çƒ­ç‚¹äº‹ä»¶åˆ—è¡¨
        """
        logger.info("é‡‡é›†çƒ­ç‚¹äº‹ä»¶...")

        all_events = []

        # æµè§ˆå™¨é‡‡é›†
        if use_browser and self.browser_collector:
            try:
                browser_events = self.browser_collector.collect_all_browser_sources()
                all_events.extend(browser_events)
                logger.info(f"æµè§ˆå™¨é‡‡é›†åˆ° {len(browser_events)} ä¸ªäº‹ä»¶")
            except Exception as e:
                logger.error(f"æµè§ˆå™¨é‡‡é›†å¤±è´¥: {e}")

        # å¦‚æœæµè§ˆå™¨é‡‡é›†å¤±è´¥æˆ–æœªä½¿ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        if len(all_events) < 5:
            logger.warning("æµè§ˆå™¨é‡‡é›†æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨è¡¥å……æ•°æ®")
            sample_titles = [
                "äººå·¥æ™ºèƒ½å¤§æ¨¡å‹å†è·çªç ´ï¼Œè¡Œä¸šè¿æ¥æ–°å˜é©",
                "æ–°èƒ½æºæ±½è½¦é”€é‡æŒç»­å¢é•¿ï¼Œå¸‚åœºæ ¼å±€ç”Ÿå˜",
                "æˆ¿åœ°äº§å¸‚åœºæ”¿ç­–è°ƒæ•´ï¼Œä¹°æˆ¿æ—¶æœºå¼•å…³æ³¨",
                "ç§‘æŠ€å·¨å¤´å‘å¸ƒæ–°å“ï¼Œå¼•é¢†è¡Œä¸šå‘å±•æ–°è¶‹åŠ¿",
                "ç¤¾ä¼šçƒ­ç‚¹äº‹ä»¶å¼•å‘å¹¿æ³›è®¨è®ºï¼Œèˆ†è®ºæŒç»­å‘é…µ",
                "å›½é™…å½¢åŠ¿å¤æ‚å¤šå˜ï¼Œç»æµå½±å“é€æ­¥æ˜¾ç°",
                "5Gç½‘ç»œå•†ç”¨åŠ é€Ÿï¼Œäº§ä¸šæ•°å­—åŒ–è½¬å‹",
                "äº’è”ç½‘å¹³å°ç›‘ç®¡åŠ å¼ºï¼Œè§„èŒƒè¡Œä¸šå‘å±•",
            ]

            for i in range(min(limit, len(sample_titles))):
                event = HotEvent(
                    event_id=f"evt_{datetime.now().strftime('%Y%m%d')}_{i:04d}",
                    title=sample_titles[i],
                    source=random.choice(["36æ°ª", "è™å—…", "æ–°æµª", "è…¾è®¯"]),
                    publish_time=datetime.now() - timedelta(hours=random.randint(1, 48)),
                    url=f"https://example.com/news/{i}.html",
                    keywords=self._extract_keywords(sample_titles[i]),
                    heat_score=random.uniform(60, 100),
                    sentiment=random.choice(["positive", "neutral", "negative"]),
                    collection_method="sample"
                )
                all_events.append(event)

        # æŒ‰çƒ­åº¦æ’åº
        all_events.sort(key=lambda x: x.heat_score, reverse=True)

        logger.info(f"é‡‡é›†å®Œæˆï¼Œå…± {len(all_events)} ä¸ªäº‹ä»¶")
        return all_events[:limit]

    def _extract_keywords(self, title: str) -> List[str]:
        """æå–å…³é”®è¯"""
        keywords = []
        keyword_list = ["äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹", "æ–°èƒ½æº", "æˆ¿åœ°äº§", "ç§‘æŠ€", "æ”¿ç­–", "ç»æµ"]
        for keyword in keyword_list:
            if keyword in title:
                keywords.append(keyword)
        return keywords if keywords else ["çƒ­ç‚¹"]

    def build_enhanced_knowledge_graph(
        self,
        events: List[HotEvent],
        topic: str = "çƒ­ç‚¹äº‹ä»¶çŸ¥è¯†å›¾è°±"
    ) -> Dict:
        """
        æ„å»ºå¢å¼ºç‰ˆçŸ¥è¯†å›¾è°±ï¼ˆåŒ…å«åµŒå…¥åˆ†æï¼‰

        å‚æ•°:
            events: çƒ­ç‚¹äº‹ä»¶åˆ—è¡¨
            topic: å›¾è°±ä¸»é¢˜

        è¿”å›:
            åŒ…å«åµŒå…¥åˆ†æçš„å¢å¼ºå›¾è°±æ•°æ®
        """
        logger.info("æ„å»ºå¢å¼ºç‰ˆçŸ¥è¯†å›¾è°±...")

        # 1. æ„å»ºåŸºç¡€å›¾è°±æ•°æ®
        nodes = []
        edges = []
        phenomena = []
        psychologies = []

        # ç°è±¡å±‚èŠ‚ç‚¹
        phenomenon_names = ["æŠ€æœ¯æ™®åŠ", "å¸‚åœºå…³æ³¨", "æ”¿ç­–æ”¯æŒ", "èµ„æœ¬æŠ•å…¥"]
        for i, name in enumerate(phenomenon_names):
            nodes.append({
                "node_id": f"phenomenon_{i}",
                "node_type": "phenomenon",
                "name": name,
                "description": f"{name}ç›¸å…³ç°è±¡åˆ†æ",
                "importance": 0.7
            })
            phenomena.append({"name": name, "id": f"phenomenon_{i}"})

        # å¿ƒç†å±‚èŠ‚ç‚¹
        emotion_names = ["ç§¯æä¹è§‚", "æœŸå¾…", "å…´å¥‹", "ç„¦è™‘"]
        for i, name in enumerate(emotion_names):
            nodes.append({
                "node_id": f"emotion_{i}",
                "node_type": "psychology",
                "name": name,
                "description": f"å…¬ä¼—{name}æƒ…ç»ª",
                "importance": 0.6
            })
            psychologies.append({"name": name, "id": f"emotion_{i}"})

        # äº‹ä»¶å±‚èŠ‚ç‚¹
        for i, event in enumerate(events[:10]):
            nodes.append({
                "node_id": f"event_{i}",
                "node_type": "event",
                "name": event.title[:30] + "...",
                "description": event.summary,
                "importance": event.heat_score / 100.0,
                "category": event.categories.get("primary", "ç»¼åˆ"),
                "keywords": event.keywords,
                "heat_score": event.heat_score,
                "source": event.source,
                "collection_method": event.collection_method
            })

            # æ·»åŠ å…³ç³»
            for j, ph in enumerate(phenomena):
                edges.append({
                    "source": f"event_{i}",
                    "target": ph["id"],
                    "relationship": "leads_to",
                    "weight": 0.8
                })

            for j, psy in enumerate(psychologies):
                edges.append({
                    "source": ph["id"],
                    "target": psy["id"],
                    "relationship": "influences",
                    "weight": 0.7
                })

        # 2. çŸ¥è¯†å›¾è°±åµŒå…¥åˆ†æ
        embedding_analysis = {}
        if self.embedder and events:
            # æ„å»ºåµŒå…¥
            self.embedder.add_entity(
                "topic", topic, "event",
                {"description": "çŸ¥è¯†å›¾è°±ä¸»é¢˜"}
            )

            # æ·»åŠ äº‹ä»¶å®ä½“
            for i, event in enumerate(events[:10]):
                self.embedder.add_entity(
                    f"event_{i}",
                    event.title[:20],
                    "event",
                    {
                        "category": event.categories.get("primary", ""),
                        "keywords": event.keywords,
                        "heat_score": event.heat_score,
                        "source": event.source,
                        "collection_method": event.collection_method
                    }
                )

            # æ·»åŠ ç°è±¡å’Œå¿ƒç†å®ä½“
            for i, ph in enumerate(phenomena):
                self.embedder.add_entity(
                    f"phenomenon_{i}",
                    ph["name"],
                    "phenomenon"
                )

            for i, psy in enumerate(psychologies):
                self.embedder.add_entity(
                    f"emotion_{i}",
                    psy["name"],
                    "psychology"
                )

            # å»ºç«‹å…³ç³»
            for i in range(10):
                for j, ph in enumerate(phenomena):
                    self.embedder.add_relation(
                        f"event_{i}", ph["id"], "leads_to"
                    )
                for j, psy in enumerate(psychologies):
                    self.embedder.add_relation(
                        ph["id"], psy["id"], "influences"
                    )

            # è®¡ç®—ç›¸ä¼¼åº¦
            event_similarities = []
            for i in range(min(10, len(events))):
                for j in range(i+1, min(10, len(events))):
                    sim = self.embedder.get_similarity(
                        f"event_{i}", f"event_{j}", method='cosine'
                    )
                    event_similarities.append({
                        "event_1": events[i].title[:20],
                        "event_2": events[j].title[:20],
                        "similarity": round(sim, 4)
                    })

            # äº‹ä»¶èšç±»
            clusters = self.embedder.find_clusters('event', n_clusters=2)
            cluster_analysis = {}
            for cid, eids in clusters.items():
                cluster_events = []
                for eid in eids:
                    try:
                        idx = int(eid.split('_')[1])
                        if 0 <= idx < len(events):
                            cluster_events.append(events[idx].title)
                    except (ValueError, IndexError):
                        continue
                cluster_analysis[f"cluster_{cid}"] = cluster_events

            # é¢„æµ‹é“¾æ¥
            predictions = self.embedder.predict_links(
                "event_0",
                [f"event_{i}" for i in range(1, min(10, len(events)))],
                top_k=3
            )

            embedding_analysis = {
                "event_similarities": sorted(
                    event_similarities,
                    key=lambda x: x["similarity"],
                    reverse=True
                )[:20],
                "clusters": cluster_analysis,
                "predictions": [
                    {
                        "event": self.embedder.entities.get(pred[0], {}).name,
                        "score": round(pred[2], 4)
                    }
                    for pred in predictions
                ],
                "statistics": self.embedder.get_statistics()
            }

        # 3. ç»Ÿè®¡æµè§ˆå™¨é‡‡é›†è¦†ç›–ç‡
        browser_count = sum(1 for e in events if e.collection_method == "browser")

        # æ„å»ºæœ€ç»ˆå›¾è°±æ•°æ®
        graph_data = {
            "graph_id": f"graph_{datetime.now().strftime('%Y%m%d%H%M%S')}",
            "topic": topic,
            "generated_time": datetime.now().isoformat(),
            "nodes": nodes,
            "edges": edges,
            "embedding_analysis": embedding_analysis,
            "statistics": {
                "event_count": len(events),
                "phenomenon_count": len(phenomena),
                "psychology_count": len(psychologies),
                "node_count": len(nodes),
                "edge_count": len(edges),
                "embedding_available": EMBEDDING_AVAILABLE,
                "browser_collection": {
                    "enabled": BROWSER_COLLECTOR_AVAILABLE,
                    "events_collected": browser_count,
                    "coverage": f"{browser_count/len(events)*100:.1f}%" if events else "0%"
                }
            }
        }

        logger.info(f"å¢å¼ºç‰ˆçŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ")
        logger.info(f"  èŠ‚ç‚¹: {len(nodes)}")
        logger.info(f"  è¾¹: {len(edges)}")
        logger.info(f"  æµè§ˆå™¨é‡‡é›†: {browser_count} æ¡ ({browser_count/len(events)*100:.1f}%)" if events else "  æµè§ˆå™¨é‡‡é›†: 0 æ¡")

        return graph_data

    def export_enhanced_graph(
        self,
        graph_data: Dict,
        output_file: str = "enhanced_knowledge_graph.md",
        format: str = "mermaid"
    ) -> str:
        """
        å¯¼å‡ºå¢å¼ºç‰ˆçŸ¥è¯†å›¾è°±

        å‚æ•°:
            graph_data: å›¾è°±æ•°æ®
            output_file: è¾“å‡ºæ–‡ä»¶å
            format: è¾“å‡ºæ ¼å¼ (mermaid, json)

        è¿”å›:
            è¾“å‡ºå†…å®¹
        """
        if format == "mermaid":
            content = self._export_mermaid(graph_data)
        else:
            content = json.dumps(graph_data, ensure_ascii=False, indent=2)

        output_path = self.output_dir / output_file
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)

        logger.info(f"çŸ¥è¯†å›¾è°±å·²ä¿å­˜åˆ° {output_path}")
        return content

    def _export_mermaid(self, graph_data: Dict) -> str:
        """å¯¼å‡ºä¸ºMermaidæ ¼å¼"""
        lines = [
            f"# å¢å¼ºç‰ˆçŸ¥è¯†å›¾è°±: {graph_data['topic']}",
            f"**ç”Ÿæˆæ—¶é—´**: {graph_data['generated_time']}",
            f"**èŠ‚ç‚¹æ•°**: {graph_data['statistics']['node_count']} | **è¾¹æ•°**: {graph_data['statistics']['edge_count']}",
            f"**æµè§ˆå™¨é‡‡é›†**: {graph_data['statistics']['browser_collection']['events_collected']} æ¡ ({graph_data['statistics']['browser_collection']['coverage']})",
            "",
            "```mermaid",
            "graph TB",
        ]

        # èŠ‚ç‚¹å®šä¹‰
        for node in graph_data['nodes']:
            node_id = node['node_id']
            name = node['name'].replace('"', "'")

            if node['node_type'] == 'event':
                lines.append(f'    {node_id}["{name}"]')
            elif node['node_type'] == 'phenomenon':
                lines.append(f'    {node_id}("{name}")')
            else:
                lines.append(f'    {node_id}<"{name}">')

        lines.append("")
        lines.append("    %% è¾¹å…³ç³»")

        for edge in graph_data['edges'][:50]:
            lines.append(
                f"    {edge['source']} -.->|{edge['relationship']}| {edge['target']}"
            )

        lines.append("")
        lines.append("    %% èŠ‚ç‚¹æ ·å¼")
        lines.append("    classDef event fill:#e1f5fe,stroke:#01579b")
        lines.append("    classDef phenomenon fill:#fff3e0,stroke:#e65100")
        lines.append("    classDef psychology fill:#f3e5f5,stroke:#4a148c")

        # åº”ç”¨æ ·å¼
        event_nodes = [n['node_id'] for n in graph_data['nodes'] if n['node_type'] == 'event']
        if event_nodes:
            lines.append(f"    class {','.join(event_nodes)} event")

        phenomenon_nodes = [n['node_id'] for n in graph_data['nodes'] if n['node_type'] == 'phenomenon']
        if phenomenon_nodes:
            lines.append(f"    class {','.join(phenomenon_nodes)} phenomenon")

        psych_nodes = [n['node_id'] for n in graph_data['nodes'] if n['node_type'] == 'psychology']
        if psych_nodes:
            lines.append(f"    class {','.join(psych_nodes)} psychology")

        lines.append("```")

        # æ·»åŠ åµŒå…¥åˆ†æç»“æœ
        if graph_data.get('embedding_analysis'):
            lines.extend([
                "",
                "---",
                "",
                "## ğŸ“Š åµŒå…¥åˆ†æç»“æœ",
                "",
                "### äº‹ä»¶ç›¸ä¼¼åº¦ TOP 10",
                "",
                "| äº‹ä»¶1 | äº‹ä»¶2 | ç›¸ä¼¼åº¦ |",
                "|-------|-------|--------|",
            ])

            for sim in graph_data['embedding_analysis'].get('event_similarities', [])[:10]:
                lines.append(
                    f"| {sim['event_1']} | {sim['event_2']} | {sim['similarity']} |"
                )

            # èšç±»ç»“æœ
            clusters = graph_data['embedding_analysis'].get('clusters', {})
            if clusters:
                lines.extend([
                    "",
                    "### äº‹ä»¶èšç±»",
                    "",
                ])
                for cid, events in clusters.items():
                    lines.append(f"- **{cid}**: {', '.join(events)}")

            # é¢„æµ‹
            predictions = graph_data['embedding_analysis'].get('predictions', [])
            if predictions:
                lines.extend([
                    "",
                    "### é“¾æ¥é¢„æµ‹",
                    "",
                    "| é¢„æµ‹äº‹ä»¶ | åˆ†æ•° |",
                    "|---------|------|",
                ])
                for pred in predictions:
                    lines.append(f"| {pred['event']} | {pred['score']} |")

        return '\n'.join(lines)


def demo():
    """æ¼”ç¤º"""
    import random

    print("=" * 80)
    print("å¢å¼ºç‰ˆçƒ­ç‚¹Agentæ¼”ç¤º - é›†æˆæµè§ˆå™¨é‡‡é›† + çŸ¥è¯†å›¾è°±åµŒå…¥")
    print("=" * 80)

    # åˆ›å»ºAgent
    agent = EnhancedHotTopicAgent()

    # é‡‡é›†äº‹ä»¶ï¼ˆä½¿ç”¨æµè§ˆå™¨é‡‡é›†ï¼‰
    print("\n[1/4] é‡‡é›†çƒ­ç‚¹äº‹ä»¶ï¼ˆæµè§ˆå™¨é‡‡é›†ï¼‰...")
    events = agent.collect_events(limit=12, use_browser=True)
    print(f"  é‡‡é›†äº† {len(events)} ä¸ªäº‹ä»¶")

    # ç»Ÿè®¡é‡‡é›†æ¥æº
    browser_count = sum(1 for e in events if e.collection_method == "browser")
    print(f"  æµè§ˆå™¨é‡‡é›†: {browser_count} æ¡")

    # æ„å»ºå¢å¼ºç‰ˆçŸ¥è¯†å›¾è°±
    print("\n[2/4] æ„å»ºå¢å¼ºç‰ˆçŸ¥è¯†å›¾è°±...")
    graph_data = agent.build_enhanced_knowledge_graph(events, "çƒ­ç‚¹äº‹ä»¶åˆ†æï¼ˆå«æµè§ˆå™¨é‡‡é›†ï¼‰")
    print(f"  èŠ‚ç‚¹: {graph_data['statistics']['node_count']}")
    print(f"  è¾¹: {graph_data['statistics']['edge_count']}")

    # å¯¼å‡º
    print("\n[3/4] å¯¼å‡ºçŸ¥è¯†å›¾è°±...")
    content = agent.export_enhanced_graph(graph_data, "enhanced_knowledge_graph_with_browser.md")
    print(f"  å·²ä¿å­˜åˆ° output/enhanced_knowledge_graph_with_browser.md")

    # æ˜¾ç¤ºåˆ†æç»“æœ
    print("\n[4/4] åµŒå…¥åˆ†æç»“æœ:")
    if graph_data.get('embedding_analysis'):
        similarities = graph_data['embedding_analysis'].get('event_similarities', [])[:5]
        print(f"  äº‹ä»¶ç›¸ä¼¼åº¦:")
        for sim in similarities:
            print(f"    {sim['event_1']} <-> {sim['event_2']}: {sim['similarity']}")

        clusters = graph_data['embedding_analysis'].get('clusters', {})
        print(f"\n  äº‹ä»¶èšç±»:")
        for cid, evts in clusters.items():
            print(f"    {cid}: {evts}")

    print("\n" + "=" * 80)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    demo()
