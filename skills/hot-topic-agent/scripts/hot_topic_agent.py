#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Hot Topic Agent Skill - Demo

çƒ­ç‚¹å¤´æ¡AgentæŠ€èƒ½æ¼”ç¤º

åŠŸèƒ½ï¼š
- å¤šæºçƒ­ç‚¹æ•°æ®é‡‡é›†ï¼ˆå¾®åšã€çŸ¥ä¹ã€æŠ–éŸ³ã€Bç«™ç­‰ï¼‰
- è¶‹åŠ¿åˆ†æ
- æƒ…æ„Ÿåˆ†æ
- çŸ¥è¯†å›¾è°±ç”Ÿæˆ

ä½œè€…: OpenClaw Agent
åˆ›å»ºæ—¶é—´: 2026-02-10
"""

import os
import sys
import json
import random
import time
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from collections import defaultdict

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class HotTopic:
    """çƒ­ç‚¹è¯é¢˜"""
    topic_id: str
    title: str
    platform: str
    heat_score: float  # 0-100
    velocity: str  # rising, stable, falling
    sentiment: str  # positive, neutral, negative
    keywords: List[str] = field(default_factory=list)
    related_topics: List[str] = field(default_factory=list)
    publish_time: str = field(default_factory=lambda: datetime.now().isoformat())


class HotTopicAgent:
    """çƒ­ç‚¹å¤´æ¡Agent"""

    # æ”¯æŒçš„å¹³å°
    PLATFORMS = {
        "weibo": {"name": "å¾®åš", "update_freq": "å®æ—¶", "quality": "é«˜"},
        "zhihu": {"name": "çŸ¥ä¹", "update_freq": "å°æ—¶çº§", "quality": "é«˜"},
        "douyin": {"name": "æŠ–éŸ³", "update_freq": "å®æ—¶", "quality": "é«˜"},
        "bilibili": {"name": "å“”å“©å“”å“©", "update_freq": "å®æ—¶", "quality": "é«˜"},
        "xiaohongshu": {"name": "å°çº¢ä¹¦", "update_freq": "å°æ—¶çº§", "quality": "ä¸­"},
        "twitter": {"name": "Twitter/X", "update_freq": "å®æ—¶", "quality": "é«˜"},
        "hackernews": {"name": "Hacker News", "update_freq": "10åˆ†é’Ÿ", "quality": "é«˜"},
        "reddit": {"name": "Reddit", "update_freq": "å®æ—¶", "quality": "ä¸­"},
    }

    def __init__(self):
        """åˆå§‹åŒ–Agent"""
        logger.info("Hot Topic Agent åˆå§‹åŒ–å®Œæˆ")
        self.topics: List[HotTopic] = []
        self._generate_sample_data()

    def _generate_sample_data(self):
        """ç”Ÿæˆç¤ºä¾‹æ•°æ®ç”¨äºæ¼”ç¤º"""
        sample_topics = [
            ("AIå¤§æ¨¡å‹å†è·çªç ´ï¼Œè¡Œä¸šè¿æ¥æ–°å˜é©", "weibo", "ç§‘æŠ€", 95, ["AI", "å¤§æ¨¡å‹", "çªç ´"]),
            ("æ–°èƒ½æºæ±½è½¦é”€é‡æŒç»­å¢é•¿ï¼Œå¸‚åœºæ ¼å±€ç”Ÿå˜", "weibo", "è´¢ç»", 92, ["æ–°èƒ½æº", "æ±½è½¦"]),
            ("æˆ¿åœ°äº§å¸‚åœºæ”¿ç­–è°ƒæ•´ï¼Œä¹°æˆ¿æ—¶æœºå¼•å…³æ³¨", "zhihu", "è´¢ç»", 91, ["æˆ¿åœ°äº§", "æ”¿ç­–"]),
            ("ç§‘æŠ€å·¨å¤´å‘å¸ƒæ–°å“ï¼Œå¼•é¢†è¡Œä¸šå‘å±•æ–°è¶‹åŠ¿", "douyin", "ç§‘æŠ€", 90, ["ç§‘æŠ€", "æ–°å“"]),
            ("ç¤¾ä¼šçƒ­ç‚¹äº‹ä»¶å¼•å‘å¹¿æ³›è®¨è®ºï¼Œèˆ†è®ºæŒç»­å‘é…µ", "twitter", "ç¤¾ä¼š", 89, ["çƒ­ç‚¹", "è®¨è®º"]),
            ("å›½é™…å½¢åŠ¿å¤æ‚å¤šå˜ï¼Œç»æµå½±å“é€æ­¥æ˜¾ç°", "reddit", "å›½é™…", 88, ["å›½é™…", "ç»æµ"]),
            ("5Gç½‘ç»œå•†ç”¨åŠ é€Ÿï¼Œäº§ä¸šæ•°å­—åŒ–è½¬å‹", "hackernews", "ç§‘æŠ€", 87, ["5G", "æ•°å­—åŒ–"]),
            ("äº’è”ç½‘å¹³å°ç›‘ç®¡åŠ å¼ºï¼Œè§„èŒƒè¡Œä¸šå‘å±•", "bilibili", "ç§‘æŠ€", 86, ["ç›‘ç®¡", "äº’è”ç½‘"]),
            ("èŠ¯ç‰‡æŠ€æœ¯è‡ªä¸»å¯æ§æˆä¸ºç„¦ç‚¹", "xiaohongshu", "ç§‘æŠ€", 85, ["èŠ¯ç‰‡", "è‡ªä¸»"]),
            ("æ•°å­—ç»æµè“¬å‹ƒå‘å±•ï¼Œæ–°ä¸šæ€ä¸æ–­æ¶Œç°", "weibo", "ç»æµ", 84, ["æ•°å­—ç»æµ", "æ–°ä¸šæ€"]),
            ("ç›´æ’­å¸¦è´§è§„èŒƒåŒ–ï¼Œè¡Œä¸šå‘å±•è¿›å…¥æ–°é˜¶æ®µ", "douyin", "ç”µå•†", 83, ["ç›´æ’­å¸¦è´§", "è§„èŒƒ"]),
            ("å…ƒå®‡å®™æ¦‚å¿µæŒç»­å‡æ¸©ï¼Œåº”ç”¨åœºæ™¯ä¸æ–­æ‹“å±•", "zhihu", "ç§‘æŠ€", 82, ["å…ƒå®‡å®™", "VR"]),
            ("ç¢³ä¸­å’Œç›®æ ‡æ¨åŠ¨æ–°èƒ½æºäº§ä¸šå¿«é€Ÿå‘å±•", "hackernews", "ç¯ä¿", 81, ["ç¢³ä¸­å’Œ", "æ–°èƒ½æº"]),
            ("AIç»˜ç”»å·¥å…·å¤§ç«ï¼Œåˆ›ä½œè€…ç”Ÿæ€é¢ä¸´å˜é©", "twitter", "ç§‘æŠ€", 80, ["AIç»˜ç”»", "åˆ›ä½œè€…"]),
            ("äº’è”ç½‘å¤§å‚å¹´ç»ˆå¥–å¼•å‘çƒ­è®®", "weibo", "èŒåœº", 79, ["å¹´ç»ˆå¥–", "å¤§å‚"]),
        ]

        sentiments = ["positive", "neutral", "negative"]
        velocities = ["rising", "stable", "falling"]

        for i, (title, platform, category, heat, keywords) in enumerate(sample_topics):
            topic = HotTopic(
                topic_id=f"topic_{i:03d}",
                title=title,
                platform=platform,
                heat_score=heat - random.uniform(0, 5),  # è½»å¾®éšæœºæ³¢åŠ¨
                velocity=random.choice(velocities),
                sentiment=random.choice(sentiments),
                keywords=keywords,
                related_topics=[sample_topics[(i + 1) % len(sample_topics)][0][:20]]
            )
            self.topics.append(topic)

    def collect_all(self, limit: int = 50) -> List[HotTopic]:
        """
        ä»æ‰€æœ‰å¹³å°é‡‡é›†çƒ­ç‚¹è¯é¢˜

        å‚æ•°:
            limit: è¿”å›æ•°é‡é™åˆ¶

        è¿”å›:
            çƒ­ç‚¹è¯é¢˜åˆ—è¡¨
        """
        logger.info(f"æ­£åœ¨é‡‡é›†æ‰€æœ‰å¹³å°çš„çƒ­ç‚¹è¯é¢˜...")
        # æ¨¡æ‹Ÿé‡‡é›†è¿‡ç¨‹
        time.sleep(0.5)
        logger.info(f"é‡‡é›†å®Œæˆï¼Œå…± {len(self.topics)} ä¸ªè¯é¢˜")
        return self.topics[:limit]

    def collect_from_platform(self, platform: str, limit: int = 20) -> List[HotTopic]:
        """
        ä»ç‰¹å®šå¹³å°é‡‡é›†çƒ­ç‚¹è¯é¢˜

        å‚æ•°:
            platform: å¹³å°åç§°
            limit: è¿”å›æ•°é‡é™åˆ¶

        è¿”å›:
            è¯¥å¹³å°çš„çƒ­ç‚¹è¯é¢˜åˆ—è¡¨
        """
        logger.info(f"æ­£åœ¨é‡‡é›† {self.PLATFORMS.get(platform, {}).get('name', platform)} çš„çƒ­ç‚¹...")
        time.sleep(0.3)
        platform_topics = [t for t in self.topics if t.platform == platform]
        logger.info(f"é‡‡é›†å®Œæˆï¼Œå…± {len(platform_topics)} ä¸ªè¯é¢˜")
        return platform_topics[:limit]

    def get_trending(self, top_k: int = 10) -> List[HotTopic]:
        """
        è·å–çƒ­é—¨æ¦œå•

        å‚æ•°:
            top_k: è¿”å›æ•°é‡

        è¿”å›:
            çƒ­åº¦æœ€é«˜çš„è¯é¢˜åˆ—è¡¨
        """
        sorted_topics = sorted(self.topics, key=lambda x: x.heat_score, reverse=True)
        return sorted_topics[:top_k]

    def analyze_sentiment(self, topic_id: str) -> Dict[str, Any]:
        """
        åˆ†æè¯é¢˜æƒ…æ„Ÿ

        å‚æ•°:
            topic_id: è¯é¢˜ID

        è¿”å›:
            æƒ…æ„Ÿåˆ†æç»“æœ
        """
        topic = next((t for t in self.topics if t.topic_id == topic_id), None)
        if not topic:
            return {"error": "è¯é¢˜ä¸å­˜åœ¨"}

        # æ¨¡æ‹Ÿæƒ…æ„Ÿåˆ†æ
        sentiment_scores = {
            "positive": random.uniform(0.6, 0.9),
            "neutral": random.uniform(0.4, 0.6),
            "negative": random.uniform(0.1, 0.4)
        }

        return {
            "topic_id": topic_id,
            "title": topic.title,
            "overall_sentiment": topic.sentiment,
            "scores": {
                "positive": sentiment_scores["positive"],
                "neutral": sentiment_scores["neutral"],
                "negative": sentiment_scores["negative"]
            },
            "emotions": {
                "joy": random.uniform(0.1, 0.5),
                "anger": random.uniform(0.0, 0.3),
                "anxiety": random.uniform(0.1, 0.4),
                "hope": random.uniform(0.3, 0.7)
            }
        }

    def predict_trends(self, hours_ahead: int = 24) -> List[Dict[str, Any]]:
        """
        é¢„æµ‹è¶‹åŠ¿èµ°å‘

        å‚æ•°:
            hours_ahead: é¢„æµ‹æ—¶é•¿ï¼ˆå°æ—¶ï¼‰

        è¿”å›:
            è¶‹åŠ¿é¢„æµ‹åˆ—è¡¨
        """
        logger.info(f"æ­£åœ¨é¢„æµ‹æœªæ¥ {hours_ahead} å°æ—¶çš„è¶‹åŠ¿...")

        # æŒ‰å…³é”®è¯åˆ†ç»„
        keyword_counts = defaultdict(float)
        for topic in self.topics:
            for keyword in topic.keywords:
                keyword_counts[keyword] += topic.heat_score

        # æ’åºå¹¶ç”Ÿæˆé¢„æµ‹
        predictions = []
        for keyword, score in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
            trend_prob = min(0.95, score / 100 + random.uniform(-0.1, 0.2))
            predictions.append({
                "keyword": keyword,
                "current_score": score,
                "prediction": "rising" if trend_prob > 0.6 else ("stable" if trend_prob > 0.4 else "falling"),
                "probability": round(trend_prob, 2),
                "hours_ahead": hours_ahead
            })

        return predictions

    def build_knowledge_graph(self, topics: List[HotTopic]) -> Dict[str, Any]:
        """
        ä»çƒ­ç‚¹è¯é¢˜æ„å»ºçŸ¥è¯†å›¾è°±

        å‚æ•°:
            topics: è¯é¢˜åˆ—è¡¨

        è¿”å›:
            çŸ¥è¯†å›¾è°±æ•°æ®
        """
        logger.info(f"æ­£åœ¨ä» {len(topics)} ä¸ªè¯é¢˜æ„å»ºçŸ¥è¯†å›¾è°±...")

        nodes = []
        edges = []
        keyword_entities = defaultdict(list)

        # åˆ›å»ºè¯é¢˜èŠ‚ç‚¹
        for topic in topics:
            nodes.append({
                "id": topic.topic_id,
                "type": "topic",
                "name": topic.title[:30],
                "attributes": {
                    "platform": topic.platform,
                    "heat_score": topic.heat_score,
                    "sentiment": topic.sentiment
                }
            })

            # è®°å½•å…³é”®è¯
            for keyword in topic.keywords:
                keyword_entities[keyword].append(topic.topic_id)

        # åˆ›å»ºå…³é”®è¯èŠ‚ç‚¹å’Œè¾¹
        keyword_id = 0
        for keyword, topic_ids in keyword_entities.items():
            keyword_node_id = f"keyword_{keyword_id:03d}"
            keyword_id += 1

            nodes.append({
                "id": keyword_node_id,
                "type": "keyword",
                "name": keyword,
                "attributes": {
                    "topic_count": len(topic_ids)
                }
            })

            # åˆ›å»ºè¾¹
            for topic_id in topic_ids:
                edges.append({
                    "source": topic_id,
                    "target": keyword_node_id,
                    "relationship": "has_keyword",
                    "weight": 1.0
                })

        # æŒ‰å¹³å°åˆ†ç»„åˆ›å»ºè¾¹
        platform_groups = defaultdict(list)
        for topic in topics:
            platform_groups[topic.platform].append(topic.topic_id)

        for platform, topic_ids in platform_groups.items():
            if len(topic_ids) > 1:
                for i in range(len(topic_ids) - 1):
                    edges.append({
                        "source": topic_ids[i],
                        "target": topic_ids[i + 1],
                        "relationship": "same_platform",
                        "weight": 0.5
                    })

        graph = {
            "graph_id": f"hot_topic_kg_{datetime.now().strftime('%Y%m%d%H%M%S')}",
            "nodes": nodes,
            "edges": edges,
            "statistics": {
                "topic_count": len(topics),
                "keyword_count": len(keyword_entities),
                "edge_count": len(edges),
                "platforms": list(set(t.platform for t in topics))
            }
        }

        logger.info(f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ: {len(nodes)} èŠ‚ç‚¹, {len(edges)} è¾¹")
        return graph

    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯

        è¿”å›:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        platform_stats = defaultdict(lambda: {"count": 0, "avg_heat": 0})
        sentiment_stats = defaultdict(int)

        for topic in self.topics:
            platform_stats[topic.platform]["count"] += 1
            platform_stats[topic.platform]["avg_heat"] += topic.heat_score
            sentiment_stats[topic.sentiment] += 1

        # è®¡ç®—å¹³å‡å€¼
        for platform in platform_stats:
            if platform_stats[platform]["count"] > 0:
                platform_stats[platform]["avg_heat"] /= platform_stats[platform]["count"]

        return {
            "total_topics": len(self.topics),
            "platforms": dict(platform_stats),
            "sentiments": dict(sentiment_stats),
            "velocity_distribution": {
                "rising": sum(1 for t in self.topics if t.velocity == "rising"),
                "stable": sum(1 for t in self.topics if t.velocity == "stable"),
                "falling": sum(1 for t in self.topics if t.velocity == "falling")
            }
        }


def demo():
    """æ¼”ç¤º"""
    print("=" * 80)
    print("Hot Topic Agent Skill æ¼”ç¤º")
    print("=" * 80)

    # åˆ›å»ºAgent
    agent = HotTopicAgent()

    # 1. é‡‡é›†æ‰€æœ‰çƒ­ç‚¹
    print("\n[1/5] é‡‡é›†çƒ­ç‚¹è¯é¢˜...")
    topics = agent.collect_all(limit=15)
    print(f"  é‡‡é›†åˆ° {len(topics)} ä¸ªçƒ­ç‚¹è¯é¢˜")

    # 2. è·å–çƒ­é—¨æ¦œå•
    print("\n[2/5] çƒ­é—¨æ¦œå• TOP 10")
    trending = agent.get_trending(top_k=10)
    platform_names = {
        "weibo": "å¾®åš", "zhihu": "çŸ¥ä¹", "douyin": "æŠ–éŸ³",
        "bilibili": "Bç«™", "twitter": "Twitter", "hackernews": "HN",
        "reddit": "Reddit", "xiaohongshu": "å°çº¢ä¹¦"
    }

    print(f"  {'æ’å':<4} {'å¹³å°':<8} {'çƒ­åº¦':<8} {'è¶‹åŠ¿':<10} {'æ ‡é¢˜'}")
    print("  " + "-" * 70)
    for i, topic in enumerate(trending, 1):
        platform = platform_names.get(topic.platform, topic.platform)
        emoji = "ğŸ“ˆ" if topic.velocity == "rising" else ("ğŸ“Š" if topic.velocity == "stable" else "ğŸ“‰")
        print(f"  {i:<4} {platform:<8} {topic.heat_score:<8.1f} {emoji} {topic.velocity:<8} {topic.title[:25]}...")

    # 3. æƒ…æ„Ÿåˆ†æç¤ºä¾‹
    print("\n[3/5] æƒ…æ„Ÿåˆ†æç¤ºä¾‹")
    if topics:
        sample_topic = topics[0]
        sentiment = agent.analyze_sentiment(sample_topic.topic_id)
        print(f"  è¯é¢˜: {sample_topic.title[:40]}...")
        print(f"  æ€»ä½“æƒ…æ„Ÿ: {sentiment['overall_sentiment']}")
        print(f"  æƒ…æ„Ÿåˆ†å¸ƒ:")
        print(f"    ç§¯æ: {sentiment['scores']['positive']:.2%}")
        print(f"    ä¸­æ€§: {sentiment['scores']['neutral']:.2%}")
        print(f"    æ¶ˆæ: {sentiment['scores']['negative']:.2%}")

    # 4. è¶‹åŠ¿é¢„æµ‹
    print("\n[4/5] è¶‹åŠ¿é¢„æµ‹")
    predictions = agent.predict_trends(hours_ahead=24)
    print(f"  {'å…³é”®è¯':<15} {'å½“å‰çƒ­åº¦':<12} {'é¢„æµ‹è¶‹åŠ¿':<12} {'æ¦‚ç‡':<8}")
    print("  " + "-" * 50)
    for pred in predictions[:5]:
        emoji = "ğŸ“ˆ" if pred["prediction"] == "rising" else ("ğŸ“Š" if pred["prediction"] == "stable" else "ğŸ“‰")
        print(f"  {pred['keyword']:<15} {pred['current_score']:<12.1f} {emoji} {pred['prediction']:<10} {pred['probability']:.0%}")

    # 5. çŸ¥è¯†å›¾è°±
    print("\n[5/5] çŸ¥è¯†å›¾è°±æ„å»º")
    graph = agent.build_knowledge_graph(topics)
    print(f"  èŠ‚ç‚¹æ•°: {len(graph['nodes'])}")
    print(f"  è¾¹æ•°: {len(graph['edges'])}")
    print(f"  å¹³å°: {', '.join(graph['statistics']['platforms'])}")

    # ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 80)
    stats = agent.get_statistics()
    print("\nğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ")
    print(f"  æ€»è¯é¢˜æ•°: {stats['total_topics']}")
    print(f"\n  å¹³å°åˆ†å¸ƒ:")
    for platform, data in stats['platforms'].items():
        name = platform_names.get(platform, platform)
        print(f"    {name}: {data['count']} ä¸ª (å¹³å‡çƒ­åº¦ {data['avg_heat']:.1f})")
    print(f"\n  æƒ…æ„Ÿåˆ†å¸ƒ:")
    for sentiment, count in stats['sentiments'].items():
        print(f"    {sentiment}: {count} ä¸ª")

    print("\n" + "=" * 80)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    demo()
