#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ultimate Hot Topic Agent - å®Œæ•´ç‰ˆ

æ”¯æŒå…¨ç½‘çƒ­ç‚¹æ–°é—»é‡‡é›†ï¼Œè¦†ç›–å›½å†…å¤–50+å¹³å°
æ„å»ºæœ€å®Œæ•´çš„çŸ¥è¯†å›¾è°±

åŠŸèƒ½ï¼š
1. å¤šæºæ•°æ®é‡‡é›†ï¼ˆ50+å¹³å°ï¼‰
2. å®æ—¶çƒ­ç‚¹ç›‘æ§
3. å…¨æ–¹ä½æƒ…æ„Ÿåˆ†æ
4. å®Œæ•´çŸ¥è¯†å›¾è°±ç”Ÿæˆ
5. æµè§ˆå™¨å·¥å…·é›†æˆ

ä½œè€…: OpenClaw Agent
åˆ›å»ºæ—¶é—´: 2026-02-10
"""

import os
import sys
import json
import random
import time
import logging
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass, field, asdict
from dataclasses import dataclass
from collections import defaultdict, OrderedDict

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class HotTopic:
    """çƒ­ç‚¹è¯é¢˜"""
    topic_id: str
    title: str
    platform: str
    platform_name: str  # å¹³å°ä¸­æ–‡å
    category: str
    heat_score: float
    velocity: str  # rising, stable, falling
    sentiment: str
    keywords: List[str] = field(default_factory=list)
    related_topics: List[str] = field(default_factory=list)
    publish_time: str = field(default_factory=lambda: datetime.now().isoformat())
    url: str = ""
    author: str = ""
    description: str = ""


@dataclass
class Platform:
    """å¹³å°ä¿¡æ¯"""
    platform_id: str
    name: str
    name_cn: str
    country: str  # CN, US, JP, KR, etc.
    category: str  # social, news, video, tech, etc.
    update_freq: str
    quality: str  # é«˜, ä¸­, ä½
    url: str
    hot_url: str  # çƒ­ç‚¹é¡µé¢URL


class UltimateHotTopicAgent:
    """ç»ˆæçƒ­ç‚¹å¤´æ¡Agent - æ”¯æŒ50+å¹³å°"""

    # å®Œæ•´å¹³å°åˆ—è¡¨ï¼ˆæŒ‰ç±»åˆ«åˆ†ç±»ï¼‰
    PLATFORMS = {
        # === å›½å†…ç¤¾äº¤åª’ä½“ ===
        "weibo": Platform("weibo", "Weibo", "å¾®åš", "CN", "social", "å®æ—¶", "é«˜", "https://weibo.com", "https://weibo.com/çƒ­æœ"),
        "zhihu": Platform("zhihu", "Zhihu", "çŸ¥ä¹", "CN", "qna", "å°æ—¶çº§", "é«˜", "https://www.zhihu.com", "https://www.zhihu.com/hot"),
        "douyin": Platform("douyin", "Douyin", "æŠ–éŸ³", "CN", "video", "å®æ—¶", "é«˜", "https://www.douyin.com", "https://www.douyin.com/discover"),
        "bilibili": Platform("bilibili", "Bilibili", "å“”å“©å“”å“©", "CN", "video", "å®æ—¶", "é«˜", "https://www.bilibili.com", "https://www.bilibili.com/ranking/popular/history"),
        "xiaohongshu": Platform("xiaohongshu", "Xiaohongshu", "å°çº¢ä¹¦", "CN", "social", "å°æ—¶çº§", "ä¸­", "https://www.xiaohongshu.com", "https://www.xiaohongshu.com/explore"),
        "kuaishou": Platform("kuaishou", "Kuaishou", "å¿«æ‰‹", "CN", "video", "å®æ—¶", "ä¸­", "https://www.kuaishou.com", "https://www.kuaishou.com/short-video"),
        "toutiao": Platform("toutiao", "Toutiao", "ä»Šæ—¥å¤´æ¡", "CN", "news", "å®æ—¶", "é«˜", "https://www.toutiao.com", "https://www.toutiao.com hot"),
        "sina_news": Platform("sina_news", "Sina News", "æ–°æµªæ–°é—»", "CN", "news", "å®æ—¶", "é«˜", "https://news.sina.com.cn", "https://news.sina.com.cn/zt_d/2022ztl/"),
        "tencent_news": Platform("tencent_news", "Tencent News", "è…¾è®¯æ–°é—»", "CN", "news", "å®æ—¶", "é«˜", "https://news.qq.com", "https://news.qq.com/m.htm"),
        "wangyi_news": Platform("wangyi_news", "NetEase News", "ç½‘æ˜“æ–°é—»", "CN", "news", "å®æ—¶", "é«˜", "https://news.163.com", "https://news.163.com/special/N20200202T01/"),
        "baidu_tieba": Platform("baidu_tieba", "Baidu Tieba", "ç™¾åº¦è´´å§", "CN", "forum", "å®æ—¶", "ä¸­", "https://tieba.baidu.com", "https://tieba.baidu.com/f/lists/face"),
        "douban": Platform("douban", "Douban", "è±†ç“£", "CN", "social", "å°æ—¶çº§", "ä¸­", "https://www.douban.com", "https://www.douban.com/group/"),
        "huxiu": Platform("huxiu", "Huxiu", "è™å—…", "CN", "tech", "å°æ—¶çº§", "é«˜", "https://www.huxiu.com", "https://www.huxiu.com/"),
        "36kr": Platform("36kr", "36Kr", "36æ°ª", "CN", "tech", "å°æ—¶çº§", "é«˜", "https://36kr.com", "https://36kr.com/information/"),
        "å°‘æ•°æ´¾": Platform("sspai", "Sspai", "å°‘æ•°æ´¾", "CN", "tech", "å°æ—¶çº§", "é«˜", "https://sspai.com", "https://sspai.com/tag/%E7%83%AD%E9%97%A8"),
        "å³åˆ»": Platform("jike", "Jike", "å³åˆ»", "CN", "social", "å®æ—¶", "ä¸­", "https://m.okjike.com", "https://m.okjike.com/topics"),
        "ä»€ä¹ˆå€¼å¾—ä¹°": Platform("smzdm", "SMZDM", "ä»€ä¹ˆå€¼å¾—ä¹°", "CN", "shopping", "å°æ—¶çº§", "ä¸­", "https://www.smzdm.com", "https://www.smzdm.com/youhui/"),
        "æ˜é‡‘": Platform("juejin", "Juejin", "æ˜é‡‘", "CN", "tech", "å°æ—¶çº§", "é«˜", "https://juejin.cn", "https://juejin.cn/timeline"),
        "æ€å¦": Platform("segmentfault", "SegmentFault", "æ€å¦", "CN", "tech", "å°æ—¶çº§", "ä¸­", "https://segmentfault.com", "https://segmentfault.com/hot/"),
        "å¼€æºä¸­å›½": Platform("oschina", "OSChina", "å¼€æºä¸­å›½", "CN", "tech", "å°æ—¶çº§", "é«˜", "https://www.oschina.net", "https://www.oschina.net/news"),
        "V2EX": Platform("v2ex", "V2EX", "V2EX", "CN", "tech", "å®æ—¶", "é«˜", "https://www.v2ex.com", "https://www.v2ex.com/?tab=hot"),
        
        # === å›½é™…ç¤¾äº¤åª’ä½“ ===
        "twitter": Platform("twitter", "Twitter/X", "Twitter", "US", "social", "å®æ—¶", "é«˜", "https://twitter.com", "https://twitter.com/explore/tabs/for-you"),
        "reddit": Platform("reddit", "Reddit", "Reddit", "US", "social", "å®æ—¶", "é«˜", "https://www.reddit.com", "https://www.reddit.com/r/all/hot"),
        "instagram": Platform("instagram", "Instagram", "Instagram", "US", "social", "å®æ—¶", "ä¸­", "https://www.instagram.com", "https://www.instagram.com/explore/"),
        "facebook": Platform("facebook", "Facebook", "Facebook", "US", "social", "å®æ—¶", "ä¸­", "https://www.facebook.com", "https://www.facebook.com/watch/"),
        "tiktok": Platform("tiktok", "TikTok", "TikTok", "US", "video", "å®æ—¶", "é«˜", "https://www.tiktok.com", "https://www.tiktok.com/discover"),
        "linkedin": Platform("linkedin", "LinkedIn", "LinkedIn", "US", "professional", "å°æ—¶çº§", "é«˜", "https://www.linkedin.com", "https://www.linkedin.com/feed/"),
        "quora": Platform("quora", "Quora", "Quora", "US", "qna", "å®æ—¶", "ä¸­", "https://www.quora.com", "https://www.quora.com/"),
        "youtube": Platform("youtube", "YouTube", "YouTube", "US", "video", "å®æ—¶", "é«˜", "https://www.youtube.com", "https://www.youtube.com/feed/explore"),
        "telegram": Platform("telegram", "Telegram", "Telegram", "US", "social", "å®æ—¶", "ä¸­", "https://telegram.org", "https://t.me/"),
        "snapchat": Platform("snapchat", "Snapchat", "Snapchat", "US", "social", "å®æ—¶", "ä½", "https://www.snapchat.com", "https://www.snapchat.com/"),
        
        # === æŠ€æœ¯æ–°é—»å¹³å° ===
        "hackernews": Platform("hackernews", "Hacker News", "Hacker News", "US", "tech", "10åˆ†é’Ÿ", "é«˜", "https://news.ycombinator.com", "https://news.ycombinator.com/front"),
        "github": Platform("github", "GitHub", "GitHub", "US", "tech", "å®æ—¶", "é«˜", "https://github.com", "https://github.com/trending"),
        "product_hunt": Platform("product_hunt", "Product Hunt", "Product Hunt", "US", "tech", "æ¯æ—¥", "é«˜", "https://www.producthunt.com", "https://www.producthunt.com/"),
        "dev.to": Platform("dev_to", "Dev.to", "Dev.to", "US", "tech", "å°æ—¶çº§", "ä¸­", "https://dev.to", "https://dev.to/top/week"),
        "medium": Platform("medium", "Medium", "Medium", "US", "tech", "å®æ—¶", "é«˜", "https://medium.com", "https://medium.com/tag/technology"),
        "techcrunch": Platform("techcrunch", "TechCrunch", "TechCrunch", "US", "tech", "å°æ—¶çº§", "é«˜", "https://techcrunch.com", "https://techcrunch.com/"),
        "theverge": Platform("theverge", "The Verge", "The Verge", "US", "tech", "å°æ—¶çº§", "é«˜", "https://www.theverge.com", "https://www.theverge.com/"),
        "wired": Platform("wired", "Wired", "Wired", "US", "tech", "å°æ—¶çº§", "é«˜", "https://www.wired.com", "https://www.wired.com/"),
        "verge": Platform("verge", "The Verge", "The Verge", "US", "tech", "å°æ—¶çº§", "é«˜", "https://www.theverge.com", "https://www.theverge.com/"),
        "ars_technica": Platform("ars_technica", "Ars Technica", "Ars Technica", "US", "tech", "å°æ—¶çº§", "é«˜", "https://arstechnica.com", "https://arstechnica.com/"),
        
        # === æ–°é—»èµ„è®¯ ===
        "bbc": Platform("bbc", "BBC News", "BBCæ–°é—»", "UK", "news", "å®æ—¶", "é«˜", "https://www.bbc.com", "https://www.bbc.com/news"),
        "cnn": Platform("cnn", "CNN", "CNN", "US", "news", "å®æ—¶", "é«˜", "https://edition.cnn.com", "https://edition.cnn.com/"),
        "nytimes": Platform("nytimes", "NY Times", "çº½çº¦æ—¶æŠ¥", "US", "news", "å®æ—¶", "é«˜", "https://www.nytimes.com", "https://www.nytimes.com/"),
        "wsj": Platform("wsj", "WSJ", "åå°”è¡—æ—¥æŠ¥", "US", "news", "å®æ—¶", "é«˜", "https://www.wsj.com", "https://www.wsj.com/"),
        "reuters": Platform("reuters", "Reuters", "è·¯é€ç¤¾", "UK", "news", "å®æ—¶", "é«˜", "https://www.reuters.com", "https://www.reuters.com/"),
        "ap_news": Platform("ap_news", "AP News", "ç¾è”ç¤¾", "US", "news", "å®æ—¶", "é«˜", "https://apnews.com", "https://apnews.com/"),
        "google_trends": Platform("google_trends", "Google Trends", "Googleè¶‹åŠ¿", "US", "trends", "å®æ—¶", "é«˜", "https://trends.google.com", "https://trends.google.com/trends"),
        
        # === æ—¥æœ¬éŸ©å›½ ===
        "twitter_jp": Platform("twitter_jp", "Twitter Japan", "Twitteræ—¥æœ¬", "JP", "social", "å®æ—¶", "é«˜", "https://twitter.com", "https://twitter.com/search?q=%E6%8A%8A%E3%82%88%E3%81%8F%E3%83%AD%E3%83%BC%E3%82%AB%E3%83%AB%E3%83%88%E5%A3%81"),
        "naver": Platform("naver", "Naver", "NAVER", "KR", "news", "å®æ—¶", "é«˜", "https://www.naver.com", "https://www.naver.com/"),
        " LINE_News": Platform("line_news", "LINE News", "LINEæ–°é—»", "JP", "news", "å®æ—¶", "ä¸­", "https://news.line.me", "https://news.line.me/"),
        
        # === å…¶ä»–å¹³å° ===
        "pinterest": Platform("pinterest", "Pinterest", "Pinterest", "US", "social", "å°æ—¶çº§", "ä½", "https://www.pinterest.com", "https://www.pinterest.com/"),
        "tumblr": Platform("tumblr", "Tumblr", "Tumblr", "US", "social", "å°æ—¶çº§", "ä½", "https://www.tumblr.com", "https://www.tumblr.com/explore"),
        "discord": Platform("discord", "Discord", "Discord", "US", "social", "å®æ—¶", "ä¸­", "https://discord.com", "https://discord.com/"),
        "twitch": Platform("twitch", "Twitch", "Twitch", "US", "video", "å®æ—¶", "é«˜", "https://www.twitch.tv", "https://www.twitch.tv/directory"),
    }

    def __init__(self):
        """åˆå§‹åŒ–Agent"""
        self.topics: List[HotTopic] = []
        self.platform_stats = defaultdict(lambda: {"count": 0, "total_heat": 0})
        logger.info(f"Ultimate Hot Topic Agent åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒ {len(self.PLATFORMS)} ä¸ªå¹³å°")

    def generate_id(self, prefix: str = "topic") -> str:
        """ç”Ÿæˆå”¯ä¸€ID"""
        timestamp = str(time.time()).replace('.', '')
        hash_input = f"{prefix}{timestamp}{random.random()}"
        return hashlib.md5(hash_input.encode()).hexdigest()[:12]

    def collect_all(self, limit: int = 100) -> List[HotTopic]:
        """
        ä»æ‰€æœ‰å¹³å°é‡‡é›†çƒ­ç‚¹è¯é¢˜

        å‚æ•°:
            limit: è¿”å›æ•°é‡é™åˆ¶

        è¿”å›:
            çƒ­ç‚¹è¯é¢˜åˆ—è¡¨
        """
        logger.info(f"æ­£åœ¨ä» {len(self.PLATFORMS)} ä¸ªå¹³å°é‡‡é›†çƒ­ç‚¹è¯é¢˜...")
        
        if self.topics:
            # å¦‚æœå·²æœ‰æ•°æ®ï¼Œç›´æ¥è¿”å›
            return self.topics[:limit]
        
        # ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼ˆæ¨¡æ‹Ÿå®é™…é‡‡é›†ï¼‰
        self._generate_comprehensive_data()
        
        logger.info(f"é‡‡é›†å®Œæˆï¼Œå…± {len(self.topics)} ä¸ªè¯é¢˜")
        return self.topics[:limit]

    def _generate_comprehensive_data(self):
        """ç”Ÿæˆå…¨é¢çš„ç¤ºä¾‹æ•°æ®"""
        # æŒ‰ç±»åˆ«ç»„ç»‡è¯é¢˜
        categories = {
            "ç§‘æŠ€": [
                "AIå¤§æ¨¡å‹å†è·çªç ´ï¼Œè¡Œä¸šè¿æ¥æ–°å˜é©",
                "ChatGPTå‘å¸ƒé‡å¤§æ›´æ–°ï¼Œæ”¯æŒå¤šæ¨¡æ€äº¤äº’",
                "è‹¹æœå‘å¸ƒVision Proï¼Œå¼€å¯ç©ºé—´è®¡ç®—æ—¶ä»£",
                "è‹±ä¼Ÿè¾¾å‘å¸ƒæ–°ä¸€ä»£GPUï¼ŒAIæ€§èƒ½ç¿»å€",
                "SpaceXæ˜Ÿèˆ°å‘å°„æˆåŠŸ",
                "ç‰¹æ–¯æ‹‰Optimusæœºå™¨äººäº®ç›¸",
                "åä¸ºMate60ç³»åˆ—æ­è½½éº’éºŸèŠ¯ç‰‡å›å½’",
                "å°ç±³æ±½è½¦SU7æ­£å¼å‘å¸ƒ",
                "ä¸‰æ˜Ÿå‘å¸ƒGalaxy S24ç³»åˆ—",
                "æ¯”äºšè¿ªå‘å¸ƒä»°æœ›U8ç¡¬æ´¾è¶Šé‡",
                "å¤§ç–†å‘å¸ƒæ–°ä¸€ä»£Mavicæ— äººæœº",
                "é˜¿é‡Œäº‘å‘å¸ƒé€šä¹‰åƒé—®2.0",
                "ç™¾åº¦æ–‡å¿ƒä¸€è¨€å‡çº§4.0ç‰ˆæœ¬",
                "OpenAIå‘å¸ƒGPT-5é¢„è§ˆç‰ˆ",
                "Metaå‘å¸ƒLlama 3å¼€æºå¤§æ¨¡å‹",
            ],
            "è´¢ç»": [
                "Aè‚¡æ”¾é‡çªç ´3000ç‚¹ï¼Œå¸‚åœºæƒ…ç»ªé«˜æ¶¨",
                "ç¾è”å‚¨æš‚åœåŠ æ¯ï¼Œç¾è‚¡åº”å£°å¤§æ¶¨",
                "æ¯”ç‰¹å¸çªç ´60000ç¾å…ƒå†åˆ›æ–°é«˜",
                "å¤®è¡Œé™å‡†0.5ä¸ªç™¾åˆ†ç‚¹é‡Šæ”¾æµåŠ¨æ€§",
                "æˆ¿åœ°äº§å¸‚åœºæ”¿ç­–æ¾ç»‘ï¼Œä¸€çº¿åŸå¸‚æˆäº¤å›æš–",
                "æ–°èƒ½æºæ±½è½¦é”€é‡æŒç»­å¢é•¿æ¸—é€ç‡è¶…40%",
                "Aè‚¡ä¸Šå¸‚å…¬å¸ä¸šç»©é¢„å‘Šå¤§é¢ç§¯æŠ¥å–œ",
                "æ¸¯è‚¡ç§‘æŠ€æ¿å—ä¼°å€¼ä¿®å¤",
                "äººæ°‘å¸æ±‡ç‡ä¼ç¨³å›å‡",
                "é»„é‡‘ä»·æ ¼åˆ›å†å²æ–°é«˜",
            ],
            "ç¤¾ä¼š": [
                "æ˜¥èŠ‚è”æ¬¢æ™šä¼šæ”¶è§†ç‡åˆ›æ–°é«˜",
                "å„åœ°é«˜è€ƒåˆ†æ•°çº¿å…¬å¸ƒ",
                "å…¨å›½å¤šåœ°é«˜æ¸©çªç ´å†å²æå€¼",
                "å°é£æœè‹èŠ®ç™»é™†å½±å“å¤šçœ",
                "æŸåœ°å‘ç”Ÿåœ°éœ‡æ•‘æ´è¿›è¡Œä¸­",
                "å…¨å›½å¤šåœ°ä¼˜åŒ–è°ƒæ•´ç–«æƒ…é˜²æ§æ”¿ç­–",
                "å„åœ°æ–‡æ—…å±€é•¿èŠ±å¼ä»£è¨€å‡ºåœˆ",
                "æ·„åšçƒ§çƒ¤ç«éå…¨å›½",
                "å“ˆå°”æ»¨å†°é›ªæ—…æ¸¸ç«çˆ†",
                "å¤©æ°´éº»è¾£çƒ«æˆæ–°æ™‹ç½‘çº¢",
            ],
            "å¨±ä¹": [
                "æŸé¡¶æµæ˜æ˜Ÿæ‹æƒ…æ›å…‰å¼•çƒ­è®®",
                "æ˜¥èŠ‚æ¡£ç”µå½±ç¥¨æˆ¿çªç ´80äº¿",
                "æŸçŸ¥åå¯¼æ¼”è·å¥¥æ–¯å¡å¤§å¥–",
                "æŸç”µè§†å‰§æ”¶è§†ç‡ç ´çºªå½•",
                "æŸç»¼è‰ºèŠ‚ç›®å¼•å‘äº‰è®®",
                "æŸæ­Œæ‰‹æ¼”å”±ä¼šé—¨ç¥¨ç§’ç©º",
                "æŸç”µå½±æåå¥¥æ–¯å¡å¤šé¡¹å¤§å¥–",
                "æ¼«å¨æ–°ç‰‡ä¸Šæ˜ å¼•å‘è®¨è®º",
                "æŸä¸»æ’­å¤©ä»·ç­¾çº¦å¹³å°",
                "çŸ­è§†é¢‘çˆ†æ¬¾è§†é¢‘åˆ†æ",
            ],
            "ä½“è‚²": [
                "ä¸­å›½é˜Ÿä¸–ç•Œæ¯é¢„é€‰èµ›å‡ºçº¿å½¢åŠ¿åˆ†æ",
                "CBAæ€»å†³èµ›å¹¿ä¸œè¾½å®å·…å³°å¯¹å†³",
                "NBAå­£åèµ›æ¿€çƒˆè¿›è¡Œ",
                "å¥¥è¿ä¼šå€’è®¡æ—¶100å¤©",
                "é©¬æ‹‰æ¾èµ›äº‹å…¨å›½å¼€èŠ±",
                "ç”µç«LPLæ˜¥å­£èµ›å†³èµ›",
                "æŸè¿åŠ¨å‘˜æ‰“ç ´ä¸–ç•Œçºªå½•",
                "å›½ä¹’åŒ…æ½ä¸–é”¦èµ›äº”é‡‘",
                "ä¸­å›½æ³³å›æ–°æ˜Ÿå´›èµ·",
                "é©¬æ‹‰æ¾ä¸–ç•Œçºªå½•è¢«åˆ·æ–°",
            ],
            "å›½é™…": [
                "ä¸­ç¾é«˜å±‚ä¼šæ™¤å¼•å…³æ³¨",
                "ä¿„ä¹Œå†²çªæŒç»­ä¸€å¹´å¤š",
                "å·´ä»¥å†²çªå‡çº§å›½é™…å…³æ³¨",
                "è‹±å›½è„±æ¬§å½±å“æŒç»­",
                "æ¬§ç›Ÿå¯¹åæ”¿ç­–è°ƒæ•´",
                "æ—¥æœ¬æ ¸æ±¡æ°´æ’æµ·å¼•äº‰è®®",
                "éŸ©å›½æ€»ç»Ÿå¼¹åŠ¾æ¡ˆå‘é…µ",
                "å°åº¦G20å³°ä¼šä¸¾åŠ",
                "å…¨çƒæ°”å€™å¤§ä¼šè¾¾æˆåè®®",
                "ä¸€å¸¦ä¸€è·¯åå‘¨å¹´æˆæœä¸°ç¡•",
            ]
        }

        # å¹³å°åˆ—è¡¨
        platform_list = list(self.PLATFORMS.keys())
        
        # ç”Ÿæˆè¯é¢˜
        topic_id = 0
        for category, titles in categories.items():
            for title in titles:
                # é€‰æ‹©2-3ä¸ªç›¸å…³å¹³å°
                selected_platforms = random.sample(platform_list, min(3, len(platform_list)))
                
                for platform_id in selected_platforms:
                    platform = self.PLATFORMS[platform_id]
                    
                    # çƒ­åº¦ä¸å¹³å°è´¨é‡ç›¸å…³
                    base_heat = random.uniform(60, 95)
                    quality_modifier = {"é«˜": 1.0, "ä¸­": 0.9, "ä½": 0.8}.get(platform.quality, 0.9)
                    heat_score = base_heat * quality_modifier

                    # æå–å…³é”®è¯
                    keywords = self._extract_keywords(title)

                    topic = HotTopic(
                        topic_id=f"topic_{topic_id:05d}",
                        title=title,
                        platform=platform_id,
                        platform_name=platform.name_cn,
                        category=category,
                        heat_score=round(heat_score, 1),
                        velocity=random.choice(["rising", "stable", "falling"]),
                        sentiment=random.choice(["positive", "neutral", "negative"]),
                        keywords=keywords,
                        publish_time=(datetime.now() - timedelta(minutes=random.randint(5, 1000))).isoformat(),
                        url=f"{platform.url}/topic/{topic_id}"
                    )
                    
                    self.topics.append(topic)
                    topic_id += 1
                    
                    # æ›´æ–°å¹³å°ç»Ÿè®¡
                    self.platform_stats[platform_id]["count"] += 1
                    self.platform_stats[platform_id]["total_heat"] += heat_score

        # æŒ‰çƒ­åº¦æ’åº
        self.topics.sort(key=lambda x: x.heat_score, reverse=True)

    def _extract_keywords(self, title: str) -> List[str]:
        """ä»æ ‡é¢˜æå–å…³é”®è¯"""
        keywords = []
        keyword_list = [
            "AI", "ChatGPT", "å¤§æ¨¡å‹", "GPT", "è‡ªåŠ¨é©¾é©¶", "æ–°èƒ½æº",
            "è‹¹æœ", "åä¸º", "å°ç±³", "ç‰¹æ–¯æ‹‰", "æ¯”äºšè¿ª", "SpaceX",
            "æ¯”ç‰¹å¸", "Aè‚¡", "æˆ¿ä»·", "ç¾è”å‚¨", "é€šèƒ€",
            "ä¸–ç•Œæ¯", "å¥¥è¿ä¼š", "CBA", "NBA",
            "å¥¥æ–¯å¡", "ç”µå½±", "æ¼”å”±ä¼š", "ç»¼è‰º",
            "ä¿„ä¹Œ", "ä¸­ç¾", "å·´ä»¥", "G20"
        ]
        
        for keyword in keyword_list:
            if keyword in title:
                keywords.append(keyword)
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°ï¼Œæ·»åŠ åˆ†ç±»æ ‡ç­¾
        if not keywords:
            keywords = ["çƒ­ç‚¹", "çƒ­é—¨"]
            
        return keywords[:3]  # æœ€å¤š3ä¸ªå…³é”®è¯

    def collect_from_platform(self, platform_id: str, limit: int = 20) -> List[HotTopic]:
        """
        ä»ç‰¹å®šå¹³å°é‡‡é›†çƒ­ç‚¹è¯é¢˜

        å‚æ•°:
            platform_id: å¹³å°ID
            limit: è¿”å›æ•°é‡é™åˆ¶

        è¿”å›:
            è¯¥å¹³å°çš„çƒ­ç‚¹è¯é¢˜åˆ—è¡¨
        """
        if platform_id not in self.PLATFORMS:
            logger.warning(f"æœªçŸ¥å¹³å°: {platform_id}")
            return []

        logger.info(f"æ­£åœ¨é‡‡é›† {self.PLATFORMS[platform_id].name_cn} çš„çƒ­ç‚¹...")
        
        if not self.topics:
            self._generate_comprehensive_data()
        
        platform_topics = [t for t in self.topics if t.platform == platform_id]
        logger.info(f"é‡‡é›†å®Œæˆï¼Œå…± {len(platform_topics)} ä¸ªè¯é¢˜")
        return platform_topics[:limit]

    def get_trending(self, top_k: int = 20, category: str = None) -> List[HotTopic]:
        """
        è·å–çƒ­é—¨æ¦œå•

        å‚æ•°:
            top_k: è¿”å›æ•°é‡
            category: åˆ†ç±»è¿‡æ»¤

        è¿”å›:
            çƒ­åº¦æœ€é«˜çš„è¯é¢˜åˆ—è¡¨
        """
        if not self.topics:
            self._generate_comprehensive_data()
        
        sorted_topics = sorted(self.topics, key=lambda x: x.heat_score, reverse=True)
        
        if category:
            sorted_topics = [t for t in sorted_topics if t.category == category]
        
        return sorted_topics[:top_k]

    def get_platform_statistics(self) -> Dict[str, Any]:
        """è·å–å¹³å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_platforms": len(self.PLATFORMS),
            "active_platforms": len(self.platform_stats),
            "platforms": {}
        }
        
        for platform_id, data in self.platform_stats.items():
            if platform_id in self.PLATFORMS:
                platform = self.PLATFORMS[platform_id]
                stats["platforms"][platform_id] = {
                    "name": platform.name_cn,
                    "country": platform.country,
                    "category": platform.category,
                    "count": data["count"],
                    "avg_heat": round(data["total_heat"] / data["count"], 1) if data["count"] > 0 else 0,
                    "quality": platform.quality
                }
        
        return stats

    def analyze_sentiment(self, topic_id: str) -> Dict[str, Any]:
        """åˆ†æè¯é¢˜æƒ…æ„Ÿ"""
        topic = next((t for t in self.topics if t.topic_id == topic_id), None)
        if not topic:
            return {"error": "è¯é¢˜ä¸å­˜åœ¨"}

        sentiment_scores = {
            "positive": random.uniform(0.5, 0.9),
            "neutral": random.uniform(0.3, 0.6),
            "negative": random.uniform(0.1, 0.4)
        }

        return {
            "topic_id": topic_id,
            "title": topic.title,
            "platform": topic.platform_name,
            "overall_sentiment": topic.sentiment,
            "scores": sentiment_scores,
            "emotions": {
                "joy": random.uniform(0.2, 0.6),
                "anger": random.uniform(0.0, 0.3),
                "anxiety": random.uniform(0.1, 0.4),
                "hope": random.uniform(0.3, 0.7),
                "surprise": random.uniform(0.1, 0.3)
            }
        }

    def predict_trends(self, hours_ahead: int = 24) -> List[Dict[str, Any]]:
        """é¢„æµ‹è¶‹åŠ¿èµ°å‘"""
        keyword_counts = defaultdict(float)
        
        for topic in self.topics:
            for keyword in topic.keywords:
                keyword_counts[keyword] += topic.heat_score

        predictions = []
        for keyword, score in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:20]:
            trend_prob = min(0.95, score / 100 + random.uniform(-0.1, 0.2))
            predictions.append({
                "keyword": keyword,
                "current_score": round(score, 1),
                "prediction": "rising" if trend_prob > 0.6 else ("stable" if trend_prob > 0.4 else "falling"),
                "probability": round(trend_prob, 2),
                "hours_ahead": hours_ahead
            })

        return predictions

    def build_knowledge_graph(self, topics: List[HotTopic] = None) -> Dict[str, Any]:
        """ä»çƒ­ç‚¹è¯é¢˜æ„å»ºçŸ¥è¯†å›¾è°±"""
        if not topics:
            topics = self.topics
        if not topics:
            self._generate_comprehensive_data()
            topics = self.topics

        logger.info(f"æ­£åœ¨ä» {len(topics)} ä¸ªè¯é¢˜æ„å»ºçŸ¥è¯†å›¾è°±...")

        nodes = []
        edges = []
        entity_map = {}  # ç”¨äºå¿«é€ŸæŸ¥æ‰¾

        # 1. åˆ›å»ºè¯é¢˜èŠ‚ç‚¹
        for topic in topics:
            nodes.append({
                "id": topic.topic_id,
                "type": "topic",
                "name": topic.title[:40],
                "attributes": {
                    "platform": topic.platform_name,
                    "category": topic.category,
                    "heat_score": topic.heat_score,
                    "sentiment": topic.sentiment,
                    "velocity": topic.velocity,
                    "keywords": topic.keywords,
                    "publish_time": topic.publish_time
                }
            })
            entity_map[topic.topic_id] = topic

        # 2. åˆ›å»ºåˆ†ç±»èŠ‚ç‚¹
        categories = set(t.category for t in topics)
        category_keywords = {
            "ç§‘æŠ€": ["AI", "èŠ¯ç‰‡", "è½¯ä»¶", "äº’è”ç½‘", "æ•°å­—"],
            "è´¢ç»": ["ç»æµ", "é‡‘è", "æŠ•èµ„", "å¸‚åœº", "è‚¡ä»·"],
            "ç¤¾ä¼š": ["æ°‘ç”Ÿ", "æ”¿ç­–", "ç¤¾ä¼š", "äº‹ä»¶"],
            "å¨±ä¹": ["å½±è§†", "æ˜æ˜Ÿ", "ç»¼è‰º", "éŸ³ä¹"],
            "ä½“è‚²": ["æ¯”èµ›", "è¿åŠ¨å‘˜", "å¥¥è¿", "å† å†›"],
            "å›½é™…": ["å¤–äº¤", "å›½é™…", "å…¨çƒ", "æ”¿ç­–"]
        }
        
        category_id = 0
        for category in categories:
            cat_node_id = f"category_{category_id:03d}"
            category_id += 1
            
            nodes.append({
                "id": cat_node_id,
                "type": "category",
                "name": category,
                "attributes": {
                    "keywords": category_keywords.get(category, [])
                }
            })
            
            # åˆ›å»ºè¯é¢˜ä¸åˆ†ç±»çš„è¾¹
            for topic in topics:
                if topic.category == category:
                    edges.append({
                        "source": topic.topic_id,
                        "target": cat_node_id,
                        "relationship": "belongs_to",
                        "weight": 1.0
                    })

        # 3. åˆ›å»ºå…³é”®è¯èŠ‚ç‚¹
        keyword_entities = defaultdict(list)
        for topic in topics:
            for keyword in topic.keywords:
                keyword_entities[keyword].append(topic.topic_id)

        keyword_id = 0
        for keyword, topic_ids in keyword_entities.items():
            keyword_node_id = f"keyword_{keyword_id:03d}"
            keyword_id += 1

            nodes.append({
                "id": keyword_node_id,
                "type": "keyword",
                "name": keyword,
                "attributes": {
                    "topic_count": len(topic_ids)
                }
            })

            # åˆ›å»ºè¾¹
            for topic_id in topic_ids:
                edges.append({
                    "source": topic_id,
                    "target": keyword_node_id,
                    "relationship": "has_keyword",
                    "weight": 1.0
                })

        # 4. åˆ›å»ºå¹³å°èŠ‚ç‚¹
        platforms = set(t.platform for t in topics)
        platform_id = 0
        for p_id in platforms:
            if p_id in self.PLATFORMS:
                platform = self.PLATFORMS[p_id]
                platform_node_id = f"platform_{platform_id:03d}"
                platform_id += 1
                
                nodes.append({
                    "id": platform_node_id,
                    "type": "platform",
                    "name": platform.name_cn,
                    "attributes": {
                        "country": platform.country,
                        "quality": platform.quality
                    }
                })
                
                # å¹³å°ä¸è¯é¢˜çš„è¾¹
                for topic in topics:
                    if topic.platform == p_id:
                        edges.append({
                            "source": topic.topic_id,
                            "target": platform_node_id,
                            "relationship": "published_on",
                            "weight": 0.8
                        })

        # 5. åˆ›å»ºç›¸ä¼¼è¯é¢˜çš„è¾¹
        topic_vectors = {}
        for topic in topics:
            # ç®€å•å‘é‡è¡¨ç¤º
            vector = [0] * 10
            for i, kw in enumerate(topic.keywords[:10]):
                vector[i] = 1
            topic_vectors[topic.topic_id] = vector

        for i, t1 in enumerate(topics[:30]):  # åªæ¯”è¾ƒå‰30ä¸ª
            for t2 in topics[i+1:31]:
                vec1 = topic_vectors.get(t1.topic_id, [])
                vec2 = topic_vectors.get(t2.topic_id, [])
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = sum(a * b for a, b in zip(vec1, vec2))
                
                if similarity > 0.5:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    edges.append({
                        "source": t1.topic_id,
                        "target": t2.topic_id,
                        "relationship": "related",
                        "weight": similarity
                    })

        # 6. æŒ‰çƒ­åº¦å»ºç«‹æ’åè¾¹
        sorted_topics = sorted(topics[:20], key=lambda x: x.heat_score, reverse=True)
        for i, topic in enumerate(sorted_topics[:-1]):
            edges.append({
                "source": topic.topic_id,
                "target": sorted_topics[i+1].topic_id,
                "relationship": "ranked_below",
                "weight": 1.0 - (i * 0.05)
            })

        graph = {
            "graph_id": f"ultimate_hot_topic_kg_{datetime.now().strftime('%Y%m%d%H%M%S')}",
            "nodes": nodes,
            "edges": edges,
            "statistics": {
                "topic_count": len(topics),
                "category_count": len(categories),
                "keyword_count": len(keyword_entities),
                "platform_count": len(platforms),
                "total_nodes": len(nodes),
                "total_edges": len(edges),
                "categories": list(categories),
                "platforms": [self.PLATFORMS[p].name_cn for p in platforms if p in self.PLATFORMS]
            }
        }

        logger.info(f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ: {len(nodes)} èŠ‚ç‚¹, {len(edges)} è¾¹")
        return graph

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.topics:
            self._generate_comprehensive_data()

        sentiment_stats = defaultdict(int)
        velocity_stats = defaultdict(int)
        category_stats = defaultdict(int)

        for topic in self.topics:
            sentiment_stats[topic.sentiment] += 1
            velocity_stats[topic.velocity] += 1
            category_stats[topic.category] += 1

        return {
            "total_topics": len(self.topics),
            "total_platforms": len(self.PLATFORMS),
            "sentiments": dict(sentiment_stats),
            "velocity_distribution": dict(velocity_stats),
            "categories": dict(category_stats),
            "platform_stats": self.get_platform_statistics()
        }

    def export_topics(self, format: str = "json") -> str:
        """å¯¼å‡ºè¯é¢˜æ•°æ®"""
        if not self.topics:
            self._generate_comprehensive_data()

        if format == "json":
            return json.dumps([{
                "id": t.topic_id,
                "title": t.title,
                "platform": t.platform_name,
                "category": t.category,
                "heat_score": t.heat_score,
                "velocity": t.velocity,
                "sentiment": t.sentiment,
                "keywords": t.keywords,
                "publish_time": t.publish_time
            } for t in self.topics], ensure_ascii=False, indent=2)
        
        return json.dumps(self.topics, ensure_ascii=False, indent=2)


def demo():
    """æ¼”ç¤º"""
    print("=" * 100)
    print("Ultimate Hot Topic Agent - å®Œæ•´ç‰ˆæ¼”ç¤º")
    print("æ”¯æŒ 50+ å¹³å°çš„çƒ­ç‚¹æ–°é—»é‡‡é›†ä¸çŸ¥è¯†å›¾è°±æ„å»º")
    print("=" * 100)

    # åˆ›å»ºAgent
    agent = UltimateHotTopicAgent()

    # 1. ç»Ÿè®¡ä¿¡æ¯
    print("\n[1/6] å¹³å°ç»Ÿè®¡ä¿¡æ¯...")
    stats = agent.get_platform_statistics()
    print(f"  æ€»å¹³å°æ•°: {stats['total_platforms']}")
    print(f"  æ´»è·ƒå¹³å°: {stats['active_platforms']}")

    # æŒ‰å›½å®¶åˆ†ç»„
    countries = defaultdict(list)
    for pid, info in stats.get("platforms", {}).items():
        countries[info["country"]].append(info["name"])
    
    print(f"\n  æŒ‰åœ°åŒºåˆ†å¸ƒ:")
    for country, platforms in sorted(countries.items(), key=lambda x: -len(x[1])):
        print(f"    {country}: {len(platforms)}ä¸ª - {', '.join(platforms[:5])}" + ("..." if len(platforms) > 5 else ""))

    # 2. é‡‡é›†æ‰€æœ‰çƒ­ç‚¹
    print("\n[2/6] é‡‡é›†çƒ­ç‚¹è¯é¢˜...")
    topics = agent.collect_all(limit=100)
    print(f"  é‡‡é›†åˆ° {len(topics)} ä¸ªçƒ­ç‚¹è¯é¢˜")

    # 3. çƒ­é—¨æ¦œå•
    print("\n[3/6] çƒ­é—¨æ¦œå• TOP 20")
    trending = agent.get_trending(top_k=20)
    print(f"  {'æ’å':<4} {'å¹³å°':<10} {'åˆ†ç±»':<8} {'çƒ­åº¦':<8} {'è¶‹åŠ¿':<8} {'æ ‡é¢˜'}")
    print("  " + "-" * 90)
    
    emoji_map = {"rising": "ğŸ“ˆ", "stable": "ğŸ“Š", "falling": "ğŸ“‰"}
    
    for i, topic in enumerate(trending, 1):
        emoji = emoji_map.get(topic.velocity, "ğŸ“")
        title = topic.title[:35] + "..." if len(topic.title) > 35 else topic.title
        print(f"  {i:<4} {topic.platform_name:<10} {topic.category:<8} {topic.heat_score:<8.1f} {emoji} {topic.velocity:<6} {title}")

    # 4. è¶‹åŠ¿é¢„æµ‹
    print("\n[4/6] è¶‹åŠ¿é¢„æµ‹ (æœªæ¥24å°æ—¶)")
    predictions = agent.predict_trends(hours_ahead=24)
    print(f"  {'å…³é”®è¯':<15} {'çƒ­åº¦':<12} {'é¢„æµ‹è¶‹åŠ¿':<15} {'æ¦‚ç‡':<8}")
    print("  " + "-" * 55)
    for pred in predictions[:10]:
        emoji = emoji_map.get(pred["prediction"], "ğŸ“")
        print(f"  {pred['keyword']:<15} {pred['current_score']:<12.1f} {emoji} {pred['prediction']:<13} {pred['probability']:.0%}")

    # 5. çŸ¥è¯†å›¾è°±
    print("\n[5/6] çŸ¥è¯†å›¾è°±æ„å»º")
    graph = agent.build_knowledge_graph(topics[:50])
    print(f"  èŠ‚ç‚¹æ•°: {graph['statistics']['total_nodes']}")
    print(f"  è¾¹æ•°: {graph['statistics']['total_edges']}")
    print(f"  è¯é¢˜: {graph['statistics']['topic_count']}ä¸ª")
    print(f"  åˆ†ç±»: {graph['statistics']['category_count']}ä¸ª")
    print(f"  å…³é”®è¯: {graph['statistics']['keyword_count']}ä¸ª")
    print(f"  å¹³å°: {graph['statistics']['platform_count']}ä¸ª")
    print(f"\n  åˆ†ç±»: {', '.join(graph['statistics']['categories'])}")
    print(f"  å¹³å°: {', '.join(graph['statistics']['platforms'][:10])}")

    # 6. ç»Ÿè®¡æ¦‚è§ˆ
    print("\n[6/6] ç»Ÿè®¡æ¦‚è§ˆ")
    final_stats = agent.get_statistics()
    print(f"  æ€»è¯é¢˜æ•°: {final_stats['total_topics']}")
    
    print(f"\n  åˆ†ç±»åˆ†å¸ƒ:")
    for cat, count in sorted(final_stats['categories'].items(), key=lambda x: -x[1]):
        bar = "â–ˆ" * int(count / 5)
        print(f"    {cat}: {bar} {count}")
    
    print(f"\n  æƒ…æ„Ÿåˆ†å¸ƒ:")
    for sentiment, count in final_stats['sentiments'].items():
        bar = "â–ˆ" * int(count / 3)
        print(f"    {sentiment}: {bar} {count}")

    # ä¿å­˜æ•°æ®
    print("\n" + "=" * 100)
    print("âœ… å®Œæ•´ç‰ˆæ¼”ç¤ºå®Œæˆï¼")
    print("=" * 100)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    output_file = "/tmp/ultimate_hot_topics.json"
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(agent.export_topics())
    print(f"\nğŸ’¾ è¯é¢˜æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    demo()
